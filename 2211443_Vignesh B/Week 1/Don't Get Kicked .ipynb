{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "### This notebook is a solution to the kaggle challenge of Don't Get Kicked!\n",
    "\n",
    "### One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases \"kicks\".\n",
    "\n",
    "### Kicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle.\n",
    "\n",
    "### Modelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers.\n",
    "\n",
    "### The challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of the script\n",
    "### 1. Load Data\n",
    "### 2. Preprocess data - process missing values and categorical values / feature normalization / Drop features \n",
    "### 3. Principal component analysis\n",
    "### 4. Undersample the majority class(0) for training\n",
    "### 5. Random forest feature importance\n",
    "### 6. Train Logistic regression, Random forest, bagging model and a feed forward neural network\n",
    "### 7. Check correlation of the predictions of the above models \n",
    "### 8. Create an ensemble of the above classifiers\n",
    "### 9. Function to calcuate the Gini score of a set of predictions\n",
    "### 10. Create a submission file for kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Importing all libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import copy\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Bidirectional\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras\n",
    "\n",
    "import pickle\n",
    "np.random.seed(1789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Fill up missing values (primitive solution)\n",
    "- Nominal Columns: Filled with mode \n",
    "- Numeric Columns: Filled with median\n",
    "2. Convert nominal columns to one-hot\n",
    "'''\n",
    "\n",
    "    \n",
    "nominal_cols = ['Auction', 'Make', 'Trim', 'TopThreeAmericanName', 'Model', 'SubModel', 'Color', 'Transmission', 'WheelType', \n",
    "                'PRIMEUNIT', 'AUCGUART', 'Nationality', 'Size', 'VNST']\n",
    "\n",
    "num_cols = ['VehicleAge', 'WheelTypeID', 'VehOdo', 'BYRNO', 'VNZIP1', 'IsOnlineSale', 'WarrantyCost'] + ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n",
    "                        'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n",
    "                        'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
    "                        'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice']\n",
    "\n",
    "global df\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    global df\n",
    "    \n",
    "    df = dataframe\n",
    "    df = df.drop(['RefId'], axis=1)\n",
    "    return df \n",
    "\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    '''\n",
    "    This function fills in the missing values\n",
    "    Currently it's a simple solution\n",
    "    - Mode for nominal columns\n",
    "    - Median for numerical columns\n",
    "    '''\n",
    "\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    for col in nominal_cols:\n",
    "        mode = df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(mode)\n",
    "\n",
    "    return df\n",
    "\n",
    "def show_nominal_values(df):\n",
    "    \n",
    "    for col in nominal_cols:\n",
    "        print col, len(Counter(df[col])) \n",
    "\n",
    "def feature_engineering(df):\n",
    "    '''\n",
    "    - Drop PurchDate & VehYear since PurchDate = VehYear + VehicleAge -> Features correlated\n",
    "    - All the Average Prices are related => so just take the average \n",
    "    '''\n",
    "    df = df.drop(['PurchDate'], axis=1)\n",
    "    df = df.drop(['VehYear'], axis=1)\n",
    "#     df = merge_auction_ave_price(df)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def convert_nominal_cols(df):\n",
    "    '''\n",
    "    This function converts nominal cols to one-hot vectors\n",
    "    '''\n",
    "    global nominal_cols\n",
    "    df_with_dummies = pd.get_dummies(df, columns = nominal_cols)\n",
    "    \n",
    "    return df_with_dummies\n",
    "\n",
    "    \n",
    "def merge_auction_ave_price(dataframe):\n",
    "    '''\n",
    "    This function takes the average of the 8 variables of the auction average prices\n",
    "    '''\n",
    "    auction_averages = ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n",
    "                        'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n",
    "                        'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
    "                        'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice']\n",
    "                        \n",
    "    \n",
    "    dataframe['AuctionAve'] = sum(dataframe[ave] for ave in auction_averages) /len(auction_averages)\n",
    "    dataframe = dataframe.drop(auction_averages, axis=1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def apply_pca(train_dataframe, test_dataframe, n_features):\n",
    "        \n",
    "    '''\n",
    "    Function applies PCA on feature vectors and returns the trasformations and the transformation matrix\n",
    "    '''\n",
    "    pca = PCA(n_components=n_features)\n",
    "    pca.fit(np.array(train_dataframe))\n",
    "\n",
    "    train_dataframe_pca = pca.transform(train_dataframe)\n",
    "\n",
    "    test_dataframe_pca = pca.transform(test_dataframe)\n",
    "    \n",
    "    return [train_dataframe, test_dataframe, train_dataframe_pca, test_dataframe_pca, pca]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline():\n",
    "\n",
    "    def __init__(self, train_file, test_file):\n",
    "    \n",
    "        self.train_dataframe = pd.read_csv(train_file, header=0) \n",
    "        self.test_dataframe = pd.read_csv(test_file, header=0)\n",
    "        self.test_dataframe_refId = self.test_dataframe['RefId']\n",
    "        \n",
    "        self.y_train = np.array(self.train_dataframe[\"IsBadBuy\"])\n",
    "        self.train_dataframe.drop(\"IsBadBuy\", axis=1, inplace=True)\n",
    "        \n",
    "#         print set(list(self.train_dataframe)).difference(list(self.test_dataframe))\n",
    "        \n",
    "        self.preprocess_data()\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \n",
    "        categories = []\n",
    "        \n",
    "        self.train_dataframe = feature_engineering(self.train_dataframe)\n",
    "        self.test_dataframe = feature_engineering(self.test_dataframe)\n",
    "        \n",
    "        self.train_dataframe = fill_missing_values(self.train_dataframe)\n",
    "        self.test_dataframe = fill_missing_values(self.test_dataframe)\n",
    "        \n",
    "        self.train_dataframe[\"dataset\"] = \"train\"\n",
    "        self.test_dataframe[\"dataset\"] = \"test\"\n",
    "        \n",
    "        self.data = pd.concat([self.train_dataframe, self.test_dataframe])\n",
    "        self.data = convert_nominal_cols(self.data)\n",
    "        \n",
    "        self.train_dataframe = self.data[self.data[\"dataset\"] == \"train\"]\n",
    "        self.test_dataframe = self.data[self.data[\"dataset\"] == \"test\"]\n",
    "\n",
    "        self.train_dataframe.drop(\"dataset\", axis=1, inplace=True)\n",
    "        self.test_dataframe.drop(\"dataset\", axis=1, inplace=True)\n",
    "        \n",
    "        print(\"Preprocessing Data\")\n",
    "        self.train_dataframe = preprocess(self.train_dataframe)\n",
    "        \n",
    "        print(\"Preprocessing Test\")\n",
    "        self.test_dataframe = preprocess(self.test_dataframe)\n",
    "\n",
    "        # Add dummy column to test dataframe to match dimensions\n",
    "        # Quick hack: should take away\n",
    "        # \t\tself.test_dataframe['IsBadBuy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharath/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sharath/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Data\n",
      "Preprocessing Test\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('training.csv', 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(list(pipe.train_dataframe)), len(list(pipe.test_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 64007, 1: 8976})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counter(pipe.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA to reduce from 2000+ features to a 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Apply PCA\n",
    "    train_dataframe_int -> before PCA\n",
    "    train_dataframe -> After PCA -> top 500 featurees\"\"\"\n",
    "[train_dataframe_init, test_dataframe_init, train_dataframe, test_dataframe, pca] = apply_pca(pipe.train_dataframe, pipe.test_dataframe, n_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72983, 500), (48707, 500), (72983, 2337), (48707, 2337))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataframe.shape, test_dataframe.shape, train_dataframe_init.shape, test_dataframe_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to shuffle data\n",
    "def shuffle_data(x_train, y_train_zero):\n",
    "    idx = np.random.randint(len(y_train_zero), size=int(len(y_train_zero)))\n",
    "    y_train_zero = y_train_zero[idx]\n",
    "    x_train = x_train[idx, :]\n",
    "    return x_train, y_train_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling Class 0 and create train, val splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_data(train_dataframe, y_train, ratio=0.2):\n",
    "    \n",
    "    y_train_zero = y_train[y_train == 0]\n",
    "    train_dataframe_zero = train_dataframe[y_train == 0]\n",
    "\n",
    "    y_train_one = y_train[y_train == 1]\n",
    "    train_dataframe_one = train_dataframe[y_train == 1]\n",
    "\n",
    "    idx = np.random.randint(len(y_train_zero), size=int(ratio*len(y_train_zero)))\n",
    "\n",
    "    y_train_zero = y_train_zero[idx]\n",
    "    train_dataframe_zero = train_dataframe_zero[idx,:]\n",
    "\n",
    "    train_dataframe_new = np.concatenate([train_dataframe_zero, train_dataframe_one])\n",
    "    y_train_new = np.concatenate([y_train_zero, y_train_one])\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_dataframe_new, y_train_new,\n",
    "                                                        test_size=0.25, random_state=4531)\n",
    "    \n",
    "    return [train_X, val_X, train_y, val_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Undersample class 0 and shuffle data - Select 15% of data from class 0\n",
    "[train_X, val_X, train_y, val_y] = filter_data(train_dataframe, pipe.y_train, 0.15)\n",
    "[train_X_init, val_X_init, train_y_init, val_y_init] = filter_data(np.array(train_dataframe_init), pipe.y_train, 0.15)\n",
    "\n",
    "train_X, train_y = shuffle_data(train_X, train_y)\n",
    "train_X_init, train_y_init = shuffle_data(train_X_init, train_y_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13932, 500), (13932,), (13932, 2337), (13932,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_X.shape, train_y.shape, train_X_init.shape, train_y_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13932, Counter({0: 7076, 1: 6856}), Counter({0: 2410, 1: 2235}))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(train_y), Counter(train_y), Counter(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13932, 2337)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_X_init.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA on test data\n",
    "test_X_init = np.array(pipe.test_dataframe)\n",
    "test_X = pca.transform(test_X_init)\n",
    "\n",
    "# Feature normalize after PCA\n",
    "test_X_n = (test_X - np.mean(train_X, axis = 0)) / (np.std(train_X, axis=0) + 0.01)\n",
    "test_X_init_n = (test_X_init - np.mean(train_X_init, axis = 0)) / (np.std(train_X_init, axis=0) + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature normalize after PCA\n",
    "train_X_n = (train_X - np.mean(train_X, axis = 0)) / (np.std(train_X, axis=0) + 0.01)\n",
    "val_X_n = (val_X - np.mean(train_X, axis = 0)) / (np.std(train_X, axis=0) + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature normalize without PCA\n",
    "train_X_init_n = (train_X_init - np.mean(train_X_init, axis = 0)) / (np.std(train_X_init, axis=0) + 0.01)\n",
    "val_X_init_n = (val_X_init - np.mean(train_X_init, axis = 0)) / (np.std(train_X_init, axis=0) + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest, feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                              random_state=0,\n",
    "                               )\n",
    "\n",
    "forest.fit(train_X, train_y)\n",
    "\n",
    "# Calculate feature importances\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "# Sort in descending order of importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute accuaracy\n",
    "def accuracy(matrix):\n",
    "    return (np.trace(matrix)) * 1.0 / np.sum(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For K fold cross validation\n",
    "'''\n",
    "k_fold = 10\n",
    "kf_total = KFold(n_splits=k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression - choose parameters using k fold cross validation and grid search\n",
    "def logistic(train_X, train_y, val_X, val_y, weight=\"balanced\"):\n",
    "    \n",
    "    logistic = LogisticRegression(C=0.5, penalty=\"l2\", class_weight=weight, max_iter=100, verbose=1)\n",
    "    logistic_clf = GridSearchCV(estimator=logistic, param_grid=dict(C=[0.5, 0.7, 1.0], class_weight=['balanced', None]), cv=10, n_jobs=-1)\n",
    "\n",
    "    cms = [confusion_matrix(train_y[test], logistic_clf.fit(train_X[train],train_y[train]).predict(train_X[test])) for train, test in kf_total.split(train_X)]\n",
    "    print(cms)\n",
    "\n",
    "    logistic = LogisticRegression(C=logistic_clf.best_estimator_.C, penalty=\"l2\", class_weight=logistic_clf.best_estimator_.class_weight, max_iter=100, verbose=1)\n",
    "    \n",
    "    logistic.fit(train_X, train_y)\n",
    "    pred = logistic.predict(val_X)\n",
    "    print (confusion_matrix(val_y, pred))\n",
    "    print (accuracy(confusion_matrix(val_y, pred)))\n",
    "    print (f1_score(val_y, pred))\n",
    "    return logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][array([[441, 241],\n",
      "       [256, 456]]), array([[475, 233],\n",
      "       [228, 458]]), array([[419, 267],\n",
      "       [230, 477]]), array([[417, 309],\n",
      "       [205, 462]]), array([[451, 259],\n",
      "       [271, 412]]), array([[425, 293],\n",
      "       [211, 464]]), array([[453, 261],\n",
      "       [252, 427]]), array([[414, 309],\n",
      "       [191, 479]]), array([[434, 280],\n",
      "       [238, 441]]), array([[421, 274],\n",
      "       [228, 470]])]\n",
      "[LibLinear][[1491  919]\n",
      " [ 775 1460]]\n",
      "0.635306781485\n",
      "0.632856523624\n"
     ]
    }
   ],
   "source": [
    "logistic_clf = logistic(train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "[[1553  857]\n",
      " [ 854 1381]]\n",
      "0.631646932185\n",
      "0.617482673821\n"
     ]
    }
   ],
   "source": [
    "#logistic_clf = logistic(train_X_n, train_y, val_X_n, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "[[1467  943]\n",
      " [ 797 1438]]\n",
      "0.625403659849\n",
      "0.623050259965\n"
     ]
    }
   ],
   "source": [
    "#logistic_clf = logistic(train_X_init, train_y_init, val_X_init, val_y_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic_clf = logistic(train_X, train_y, val_X, val_y, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic_clf = logistic(train_X_init, train_y_init, val_X_init, val_y_init, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - choose parameters using k fold cross validation and grid search\n",
    "def run_forest(train_X, train_y, val_X, val_y):\n",
    "    forest = RandomForestClassifier(n_estimators=250,\n",
    "                                  random_state=0,\n",
    "                                   )\n",
    "    clf_forest = GridSearchCV(estimator=forest, param_grid=dict(n_estimators=[200, 350, 500], warm_start=[True, False]), cv=k_fold, n_jobs=-1)\n",
    "    cms = [confusion_matrix(train_y[test], clf_forest.fit(train_X[train],train_y[train]).predict(train_X[test])) for train, test in kf_total.split(train_X)]\n",
    "#     print(cms)\n",
    "    \n",
    "#     forest.fit(train_X, train_y)\n",
    "#     forest = \n",
    "\n",
    "    forest = RandomForestClassifier(n_estimators=clf_forest.best_estimator_.n_estimators,\n",
    "                                  warm_start=clf_forest.best_estimator_.warm_start,\n",
    "                                   )\n",
    "\n",
    "    forest.fit(train_X, train_y)\n",
    "    pred = forest.predict(val_X)\n",
    "    print(confusion_matrix(val_y, pred))\n",
    "    print(accuracy(confusion_matrix(val_y, pred)))\n",
    "    print(f1_score(val_y, pred))\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1577  833]\n",
      " [ 871 1364]]\n",
      "0.633153928956\n",
      "0.615523465704\n"
     ]
    }
   ],
   "source": [
    "#rf_clf = run_forest(train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1575  835]\n",
      " [ 876 1359]]\n",
      "0.631646932185\n",
      "0.61368254685\n"
     ]
    }
   ],
   "source": [
    "#rf_clf = run_forest(train_X_n, train_y, val_X_n, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1563  847]\n",
      " [ 817 1418]]\n",
      "0.641765339074\n",
      "0.630222222222\n"
     ]
    }
   ],
   "source": [
    "rf_clf = run_forest(train_X_init, train_y_init, val_X_init, val_y_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1567  843]\n",
      " [ 812 1423]]\n",
      "0.643702906351\n",
      "0.632303932459\n"
     ]
    }
   ],
   "source": [
    "#rf_clf = run_forest(train_X_init_n, train_y_init, val_X_init_n, val_y_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM - choose parameters using k fold cross validation and grid search\n",
    "from sklearn import svm\n",
    "def run_svm(train_X, train_y, val_X, val_y):\n",
    "\n",
    "    svc = svm.SVC(C=1.0, kernel='linear')\n",
    "    svc.fit(train_X, train_y)\n",
    "    pred = svc.predict(val_X)\n",
    "    print (confusion_matrix(val_y, pred))\n",
    "    print (accuracy(confusion_matrix(val_y, pred)))\n",
    "    print (f1_score(val_y, pred))\n",
    "    return svc\n",
    "\n",
    "svc = run_svm(train_X, train_y, val_X, val_y)\n",
    "svc_2 = run_svm(train_X_init, train_y_init, val_X_init, val_y_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1558  852]\n",
      " [ 890 1345]]\n",
      "0.624973089343\n",
      "0.606949458484\n"
     ]
    }
   ],
   "source": [
    "[svm] = pickle.load(open(\"svm.pickle\", \"r\"))\n",
    "pred = svm.predict(val_X)\n",
    "print (confusion_matrix(val_y, pred))\n",
    "print (accuracy(confusion_matrix(val_y, pred)))\n",
    "print (f1_score(val_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1639  771]\n",
      " [ 862 1373]]\n",
      "0.648439181916\n",
      "0.627083809089\n"
     ]
    }
   ],
   "source": [
    "# Bagging - choose parameters using k fold cross validation and grid search\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "# estimators_list = [150, 250, 500]\n",
    "\n",
    "# k_fold = 5\n",
    "# kf_total = KFold(n_splits=k_fold)\n",
    "\n",
    "# clf_bagging = GridSearchCV(estimator=bagging, param_grid=dict(n_estimators=estimators_list, warm_start=[True, False]), cv=k_fold, n_jobs=-1)\n",
    "# cms = [confusion_matrix(train_y[test], clf_bagging.fit(train_X[train], train_y[train]).predict(train_X[test])) for train, test in kf_total.split(train_X)]\n",
    "# accuracies = []\n",
    "# for cm in cms:\n",
    "#     accuracies.append(accuracy(cm))\n",
    "# print(accuracies)\n",
    "# print(np.mean(accuracies))\n",
    "bagging.fit(train_X, train_y)\n",
    "pred = bagging.predict(val_X)\n",
    "print (confusion_matrix(val_y, pred))\n",
    "print (accuracy(confusion_matrix(val_y, pred)))\n",
    "print (f1_score(val_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 11145 samples, validate on 2787 samples\n",
      "Epoch 1/20\n",
      "11145/11145 [==============================] - 1s 48us/step - loss: 0.9540 - acc: 0.5345 - val_loss: 0.6817 - val_acc: 0.6085\n",
      "Epoch 2/20\n",
      "11145/11145 [==============================] - 0s 44us/step - loss: 0.7319 - acc: 0.5917 - val_loss: 0.6370 - val_acc: 0.6480\n",
      "Epoch 3/20\n",
      "11145/11145 [==============================] - 0s 43us/step - loss: 0.6504 - acc: 0.6381 - val_loss: 0.6234 - val_acc: 0.6597\n",
      "Epoch 4/20\n",
      "11145/11145 [==============================] - 0s 34us/step - loss: 0.6154 - acc: 0.6645 - val_loss: 0.6160 - val_acc: 0.6659\n",
      "Epoch 5/20\n",
      "11145/11145 [==============================] - 0s 31us/step - loss: 0.5954 - acc: 0.6832 - val_loss: 0.6088 - val_acc: 0.6697\n",
      "Epoch 6/20\n",
      "11145/11145 [==============================] - 0s 40us/step - loss: 0.5806 - acc: 0.6966 - val_loss: 0.6056 - val_acc: 0.6733\n",
      "Epoch 7/20\n",
      "11145/11145 [==============================] - 0s 35us/step - loss: 0.5710 - acc: 0.7056 - val_loss: 0.6022 - val_acc: 0.6755\n",
      "Epoch 8/20\n",
      "11145/11145 [==============================] - 0s 42us/step - loss: 0.5570 - acc: 0.7170 - val_loss: 0.6015 - val_acc: 0.6781\n",
      "Epoch 9/20\n",
      "11145/11145 [==============================] - 1s 49us/step - loss: 0.5491 - acc: 0.7146 - val_loss: 0.5980 - val_acc: 0.6771\n",
      "Epoch 10/20\n",
      "11145/11145 [==============================] - 0s 31us/step - loss: 0.5451 - acc: 0.7220 - val_loss: 0.5955 - val_acc: 0.6817\n",
      "Epoch 11/20\n",
      "11145/11145 [==============================] - 0s 29us/step - loss: 0.5359 - acc: 0.7279 - val_loss: 0.5947 - val_acc: 0.6841\n",
      "Epoch 12/20\n",
      "11145/11145 [==============================] - 0s 31us/step - loss: 0.5258 - acc: 0.7361 - val_loss: 0.5951 - val_acc: 0.6875\n",
      "Epoch 13/20\n",
      "11145/11145 [==============================] - 0s 37us/step - loss: 0.5220 - acc: 0.7385 - val_loss: 0.5917 - val_acc: 0.6887\n",
      "Epoch 14/20\n",
      "11145/11145 [==============================] - 0s 38us/step - loss: 0.5202 - acc: 0.7425 - val_loss: 0.5897 - val_acc: 0.6959\n",
      "Epoch 15/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.5107 - acc: 0.7502 - val_loss: 0.5861 - val_acc: 0.6943\n",
      "Epoch 16/20\n",
      "11145/11145 [==============================] - 1s 53us/step - loss: 0.5012 - acc: 0.7538 - val_loss: 0.5854 - val_acc: 0.6973\n",
      "Epoch 17/20\n",
      "11145/11145 [==============================] - 0s 31us/step - loss: 0.4991 - acc: 0.7591 - val_loss: 0.5848 - val_acc: 0.7051\n",
      "Epoch 18/20\n",
      "11145/11145 [==============================] - 0s 31us/step - loss: 0.4963 - acc: 0.7600 - val_loss: 0.5814 - val_acc: 0.7076\n",
      "Epoch 19/20\n",
      "11145/11145 [==============================] - 0s 32us/step - loss: 0.4887 - acc: 0.7631 - val_loss: 0.5828 - val_acc: 0.7110\n",
      "Epoch 20/20\n",
      "11145/11145 [==============================] - 0s 39us/step - loss: 0.4785 - acc: 0.7729 - val_loss: 0.5830 - val_acc: 0.7079\n",
      "[0.44614967418887508, 0.80017226528854435]\n",
      "[0.72123019305189151, 0.63175457483728936]\n"
     ]
    }
   ],
   "source": [
    "# Feedforward neural network\n",
    "batch_size = 64\n",
    "\n",
    "X_train_lstm = np.reshape(train_X_n, (len(train_X), len(train_X[0])))#, 1))\n",
    "X_test_lstm = np.reshape(val_X_n, (len(val_X), len(val_X[0])))#, 1))\n",
    "\n",
    "y_train_lstm = to_categorical(train_y)\n",
    "y_test_lstm = to_categorical(val_y)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None)\n",
    ", input_shape=X_train_lstm.shape[1:]))      \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_lstm, y_train_lstm,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          #callbacks = callbacks_list,\n",
    "         )\n",
    "scores = model.evaluate(X_train_lstm, y_train_lstm, verbose=0)\n",
    "print(scores)\n",
    "scores = model.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predictions from the model\n",
    "pred = np.argmax(model.predict(X_test_lstm), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1573  837]\n",
      " [ 874 1361]]\n",
      "0.631646932185\n",
      "0.61403113016\n"
     ]
    }
   ],
   "source": [
    "print (confusion_matrix(val_y, pred))\n",
    "print (accuracy(confusion_matrix(val_y, pred)))\n",
    "print (f1_score(val_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model - Max polling and predictions between ANN, LR, RF, SVM and Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble_kaggle_submission(classifiers, data):\n",
    "\n",
    "    '''\n",
    "    Predicted is the probability of a bad buy[0-1]\n",
    "    '''\n",
    "#     classifiers = [model, logistic_clf, rf_clf, bagging]\n",
    "#     X_val_nn = np.reshape(val_X_n, (len(val_X_n), len(val_X_n[0])))#, 1))\n",
    "#     data = [val_X_n, val_X, val_X_init, val_X]\n",
    "    pred = []\n",
    "\n",
    "    for i in range(len(classifiers)):\n",
    "\n",
    "#         if i == 4 or i == 1:\n",
    "#             continue\n",
    "            \n",
    "        if i == 0:\n",
    "            pred_i = classifiers[i].predict(data[i])[:, 1]\n",
    "        else:\n",
    "            pred_i = classifiers[i].predict_proba(data[i])[:, 1]\n",
    "        \n",
    "        pred.append(pred_i)\n",
    "\n",
    "    \n",
    "    pred = np.array(pred)\n",
    "    \n",
    "    #Measure correlation between predictions of different models\n",
    "    print(np.corrcoef(pred))\n",
    "    \n",
    "    pred = np.mean(pred, axis = 0)\n",
    "#     pred[pred <= 0.5] = 0\n",
    "#     pred[pred > 0.5] = 1\n",
    "    return pred\n",
    "\n",
    "def ensemble(classifiers, data):\n",
    "    \n",
    "    '''\n",
    "    Prediction of whether it is a good or bad buy(0/1)\n",
    "    '''\n",
    "    \n",
    "#     classifiers = [model, logistic_clf, rf_clf, bagging]\n",
    "#     X_val_nn = np.reshape(val_X_n, (len(val_X_n), len(val_X_n[0])))#, 1))\n",
    "#     data = [val_X_n, val_X, val_X_init, val_X]\n",
    "    pred = []\n",
    "\n",
    "    for i in range(len(classifiers)):\n",
    "\n",
    "#         if i == 4 or i == 1:\n",
    "#             continue\n",
    "\n",
    "        if i == 0:\n",
    "            pred_i = np.argmax(classifiers[i].predict(data[i]), axis=1)\n",
    "        else:\n",
    "            pred_i = classifiers[i].predict(data[i])\n",
    "\n",
    "        pred.append(pred_i)\n",
    "\n",
    "    \n",
    "    pred = np.array(pred)\n",
    "    \n",
    "    #Measure correlation between predictions of different models\n",
    "    print(np.corrcoef(pred))\n",
    "    \n",
    "    #Average predictions and threshold them\n",
    "    pred = np.mean(pred, axis = 0)\n",
    "    pred[pred < 0.5] = 0\n",
    "    pred[pred >= 0.5] = 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   4.77264104e-01  -1.29529764e-02   9.05710383e-01]\n",
      " [  4.77264104e-01   1.00000000e+00   8.79814280e-04   3.82273311e-01]\n",
      " [ -1.29529764e-02   8.79814280e-04   1.00000000e+00  -1.59856326e-02]\n",
      " [  9.05710383e-01   3.82273311e-01  -1.59856326e-02   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pred_train = ensemble_kaggle_submission([model, logistic_clf, rf_clf, bagging, svm], [train_X_n, train_X, train_X_init, train_X, train_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.28876431  0.52686806]\n",
      " [ 0.28876431  1.          0.28039007]\n",
      " [ 0.52686806  0.28039007  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Binary prediction on val set\n",
    "pred = ensemble([model, logistic_clf, rf_clf, bagging, svm], [val_X_n, val_X, val_X_init, val_X, val_X])\n",
    "# Continuous prediction on val set\n",
    "pred_continous = ensemble_kaggle_submission([model, logistic_clf, rf_clf, bagging, svm], [val_X_n, val_X, val_X_init, val_X, val_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1473  937]\n",
      " [ 580 1655]]\n",
      "0.673412271259\n",
      "0.685726123886\n"
     ]
    }
   ],
   "source": [
    "#Validation accuracy - Ensemble of all 5 models\n",
    "print (confusion_matrix(val_y, pred))\n",
    "print (accuracy(confusion_matrix(val_y, pred)))\n",
    "print (f1_score(val_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1750  660]\n",
      " [ 756 1479]]\n",
      "0.695156081808\n",
      "0.676268861454\n"
     ]
    }
   ],
   "source": [
    "#Validation accuracy - Ensemble of RF + NN + Bagging \n",
    "print (confusion_matrix(val_y, pred))\n",
    "print (accuracy(confusion_matrix(val_y, pred)))\n",
    "print (f1_score(val_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating predictions on kaggle test set - 0.1882(LR+RF+NN+Bagging) Gini for initial submission\n",
    "## Train on train + val sets before submitting on Kaggle's test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models on train+val to predict on Kaggle's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_clf.fit(np.concatenate([train_X_n, val_X_n]), np.concatenate([train_y, val_y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(np.concatenate([train_X, val_X]), np.concatenate([train_y, val_y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11145 samples, validate on 2787 samples\n",
      "Epoch 1/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.3688 - acc: 0.8349 - val_loss: 0.3038 - val_acc: 0.8812\n",
      "Epoch 2/20\n",
      "11145/11145 [==============================] - 0s 31us/step - loss: 0.3578 - acc: 0.8397 - val_loss: 0.3115 - val_acc: 0.8762\n",
      "Epoch 3/20\n",
      "11145/11145 [==============================] - 0s 36us/step - loss: 0.3432 - acc: 0.8473 - val_loss: 0.3159 - val_acc: 0.8676\n",
      "Epoch 4/20\n",
      "11145/11145 [==============================] - 1s 47us/step - loss: 0.3403 - acc: 0.8478 - val_loss: 0.3227 - val_acc: 0.8651\n",
      "Epoch 5/20\n",
      "11145/11145 [==============================] - 0s 39us/step - loss: 0.3372 - acc: 0.8524 - val_loss: 0.3284 - val_acc: 0.8604\n",
      "Epoch 6/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.3322 - acc: 0.8540 - val_loss: 0.3333 - val_acc: 0.8601\n",
      "Epoch 7/20\n",
      "11145/11145 [==============================] - 1s 49us/step - loss: 0.3228 - acc: 0.8619 - val_loss: 0.3329 - val_acc: 0.8558\n",
      "Epoch 8/20\n",
      "11145/11145 [==============================] - 1s 48us/step - loss: 0.3221 - acc: 0.8589 - val_loss: 0.3380 - val_acc: 0.8586\n",
      "Epoch 9/20\n",
      "11145/11145 [==============================] - 0s 39us/step - loss: 0.3088 - acc: 0.8657 - val_loss: 0.3425 - val_acc: 0.8543\n",
      "Epoch 10/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.3141 - acc: 0.8617 - val_loss: 0.3461 - val_acc: 0.8529\n",
      "Epoch 11/20\n",
      "11145/11145 [==============================] - 0s 35us/step - loss: 0.3037 - acc: 0.8671 - val_loss: 0.3464 - val_acc: 0.8572\n",
      "Epoch 12/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.3137 - acc: 0.8660 - val_loss: 0.3504 - val_acc: 0.8518\n",
      "Epoch 13/20\n",
      "11145/11145 [==============================] - 0s 32us/step - loss: 0.3054 - acc: 0.8658 - val_loss: 0.3591 - val_acc: 0.8471\n",
      "Epoch 14/20\n",
      "11145/11145 [==============================] - 0s 32us/step - loss: 0.2940 - acc: 0.8731 - val_loss: 0.3610 - val_acc: 0.8479\n",
      "Epoch 15/20\n",
      "11145/11145 [==============================] - 0s 32us/step - loss: 0.2975 - acc: 0.8715 - val_loss: 0.3638 - val_acc: 0.8432\n",
      "Epoch 16/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.2941 - acc: 0.8736 - val_loss: 0.3641 - val_acc: 0.8497\n",
      "Epoch 17/20\n",
      "11145/11145 [==============================] - 0s 34us/step - loss: 0.2974 - acc: 0.8718 - val_loss: 0.3722 - val_acc: 0.8475\n",
      "Epoch 18/20\n",
      "11145/11145 [==============================] - 0s 33us/step - loss: 0.2948 - acc: 0.8739 - val_loss: 0.3788 - val_acc: 0.8461\n",
      "Epoch 19/20\n",
      "11145/11145 [==============================] - 0s 32us/step - loss: 0.2876 - acc: 0.8760 - val_loss: 0.3737 - val_acc: 0.8439\n",
      "Epoch 20/20\n",
      "11145/11145 [==============================] - 0s 32us/step - loss: 0.2853 - acc: 0.8781 - val_loss: 0.3778 - val_acc: 0.8489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2997bd990>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.concatenate([X_train_lstm, X_test_lstm]), np.concatenate([y_train_lstm, y_test_lstm]),\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          #callbacks = callbacks_list,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=250, n_jobs=1, oob_score=False, random_state=0,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging.fit(np.concatenate([train_X, val_X]), np.concatenate([train_y, val_y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.57170606  0.63946228]\n",
      " [ 0.57170606  1.          0.64364128]\n",
      " [ 0.63946228  0.64364128  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "ref_id = np.array(pipe.test_dataframe_refId)\n",
    "pred_test = ensemble_kaggle_submission([model, logistic_clf, rf_clf, bagging, svm], [test_X_n, test_X, test_X_init, test_X, test_X])\n",
    "\n",
    "assert len(ref_id) == len(pred_test)\n",
    "\n",
    "f = open(\"submission5.csv\", \"w\")\n",
    "f.write(\"RefId,IsBadBuy\\n\")\n",
    "\n",
    "for i in range(len(ref_id)):\n",
    "    if i != len(ref_id) - 1:\n",
    "        f.write(str(ref_id[i]) + \",\" + str(pred_test[i]) + \"\\n\")\n",
    "    else:\n",
    "        f.write(str(ref_id[i]) + \",\" + str(pred_test[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle evaluation metric - Gini coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89211147613541375"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gini(actual, pred):\n",
    "\n",
    "    actual_len = len(actual)\n",
    "    assert( actual_len == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(actual_len) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ] \n",
    "    giniSum = all[:,0].cumsum().sum() / all[:,0].sum()\n",
    "    giniSum -= (actual_len + 1) / 2.\n",
    "    return giniSum / actual_len\n",
    "\n",
    "def normalized_gini(solution, submission):\n",
    "    normalized_gini = gini(solution, submission)/gini(solution, solution)\n",
    "    return normalized_gini\n",
    "\n",
    "\n",
    "normalized_gini(val_y, pred_continuous)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
