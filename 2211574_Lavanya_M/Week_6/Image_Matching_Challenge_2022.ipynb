{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0968d560",
   "metadata": {
    "papermill": {
     "duration": 0.03723,
     "end_time": "2022-04-25T21:34:16.776164",
     "exception": false,
     "start_time": "2022-04-25T21:34:16.738934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# Image Matching Challenge 2022\n",
    "##### Register two images from different viewpoints\n",
    "---\n",
    "\n",
    "### **Objective**\n",
    "The [Image Matching Challenge 2022](https://www.kaggle.com/competitions/image-matching-challenge-2022/overview) Register two images from different viewpoints Kaggle competition is focused on developing computer vision models that can match pairs of images that show the same scene or object from different viewpoints. Specifically, the competition requires participants to develop algorithms that can register two images by estimating the 3D transformation that aligns them. The competition provides a dataset of image pairs along with ground-truth correspondences between the images, and participants are evaluated based on the accuracy of their registration algorithm. The goal of the competition is to promote research and development in the area of image registration, which has important applications in fields such as robotics, augmented reality, and remote sensing.\n",
    "\n",
    "\n",
    "In this notebook, I provide an overview of the competition and comprehensive solution to the Image Matching Challenge 2022 Kaggle competition, providing an end-to-end implementation for the problem statement, from data exploration and preprocessing to model training and submission.\n",
    "\n",
    "\n",
    "### **Data**\n",
    "\n",
    "[The Dataset](https://www.kaggle.com/competitions/image-matching-challenge-2022/data) for the Image Matching Challenge 2022 consists of two parts: the training dataset and the test dataset.\n",
    "\n",
    "- The training dataset contains 5,000 pairs of images, where each pair of images shows the same object from two different viewpoints. The images are stored as grayscale images in JPEG format and have a resolution of 240 x 320 pixels. Each pair of images is stored in a separate directory, with the two images named as \"left.jpg\" and \"right.jpg\".\n",
    "\n",
    "- The test dataset contains 10,000 pairs of images, with the same format as the training dataset. However, the test dataset does not have corresponding ground truth labels, which makes it challenging to evaluate the performance of different image matching algorithms.\n",
    "\n",
    "Both the training and test datasets are provided as compressed ZIP files that can be downloaded from the competition page on Kaggle.\n",
    "\n",
    "**Note:** This notebook has more than 25mb, So I can only upload code.\n",
    "\n",
    "---\n",
    "### Table of Contents\n",
    "\n",
    "- Imports       \n",
    "1. Background Information\n",
    "2. Setup\n",
    "3. Helper Functions\n",
    "4. Dataset Exploration and Preprocessing\n",
    "5. Baseline\n",
    "6. Submission\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309d2c8",
   "metadata": {
    "papermill": {
     "duration": 0.035117,
     "end_time": "2022-04-25T21:34:16.921011",
     "exception": false,
     "start_time": "2022-04-25T21:34:16.885894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77900dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:17.012082Z",
     "iopub.status.busy": "2022-04-25T21:34:16.996504Z",
     "iopub.status.idle": "2022-04-25T21:34:25.931026Z",
     "shell.execute_reply": "2022-04-25T21:34:25.930345Z",
     "shell.execute_reply.started": "2022-04-14T22:28:36.670605Z"
    },
    "papermill": {
     "duration": 8.973413,
     "end_time": "2022-04-25T21:34:25.931172",
     "exception": false,
     "start_time": "2022-04-25T21:34:16.957759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n... IMPORTS STARTING ...\\n\")\n",
    "\n",
    "print(\"\\n\\tVERSION INFORMATION\")\n",
    "\n",
    "# Machine Learning and Data Science Imports\n",
    "import tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\n",
    "import tensorflow_hub as tfhub; print(f\"\\t\\t‚Äì TENSORFLOW HUB VERSION: {tfhub.__version__}\");\n",
    "import tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\n",
    "import pandas as pd; pd.options.mode.chained_assignment = None;\n",
    "import numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\n",
    "import sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from pandarallel import pandarallel; pandarallel.initialize();\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# RAPIDS\n",
    "# import cudf, cupy, cuml\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import TSNE, UMAP\n",
    "\n",
    "# Built In Imports\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import warnings\n",
    "import requests\n",
    "import hashlib\n",
    "import imageio\n",
    "import IPython\n",
    "import sklearn\n",
    "import urllib\n",
    "import zipfile\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gzip\n",
    "import ast\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# Visualization Imports\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as patches\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm; tqdm.pandas();\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\n",
    "from matplotlib import animation, rc; rc('animation', html='jshtml')\n",
    "import plotly\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import plotly.io as pio\n",
    "print(pio.renderers)\n",
    "\n",
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    \n",
    "print(\"\\n\\n... IMPORTS COMPLETE ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f67fbf",
   "metadata": {
    "papermill": {
     "duration": 0.037285,
     "end_time": "2022-04-25T21:34:26.007027",
     "exception": false,
     "start_time": "2022-04-25T21:34:25.969742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Background Information\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540758f",
   "metadata": {
    "papermill": {
     "duration": 0.036946,
     "end_time": "2022-04-25T21:34:26.081604",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.044658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1 BASIC COMPETITION INFORMATION\n",
    "---\n",
    "\n",
    "**Participants are asked to estimate the relative pose of one image with respect to another. For each ID in the test set, you must predict the fundamental matrix between the two views.**\n",
    "\n",
    "The classical pipeline (also implemented in examples) is to\n",
    "1. Extract Local Features\n",
    "2. Match Local Features in Image 1 to Local Features in Image 2\n",
    "3. Filter These Feature Matches\n",
    "4. Apply RANSAC (Or Some Such Similar Outlier Elimination)\n",
    "\n",
    "You can train whole pipeline end-to-end, which is hard, or train in modular way...\n",
    "1. DISK for local features\n",
    "2. SuperGlue for matching\n",
    "3. OANet for filtering\n",
    "4. Apply some algorithmic improvements to RANSAC, to have better results.\n",
    "\n",
    "<a href=\"https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/318022\">[REF]</a>\n",
    "\n",
    "**The Basic Pipeline - NOTE: This Competition is Task 1**\n",
    "\n",
    " <img alt=\"iwild\" src=\"https://ducha-aiki.github.io/wide-baseline-stereo-blog/images/copied_from_nb/2021-05-14-IMC2020-competition-recap_files/att_00000.png\" width=\"650px\"/>\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">CONTEXT</b>\n",
    "\n",
    "For most of us, our best camera is part of the phone in our pocket. We may take a snap of a landmark, like the Trevi Fountain in Rome, and share it with friends. By itself, that photo is two-dimensional and only includes the perspective of our shooting location. Of course, a lot of people have taken photos of that fountain. Together, we may be able to create a more complete, three-dimensional view. What if machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet?\n",
    "\n",
    "The process to reconstruct 3D objects and buildings from images is called Structure-from-Motion (SfM). Typically, these images are captured by skilled operators under controlled conditions, ensuring homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, lighting and weather conditions, occlusions from people and vehicles, and even user-applied filters.\n",
    "\n",
    " <img alt=\"iwild\" src=\"https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/trevi-canvas-licensed-nonoderivs.jpg\" width=\"650px\"/>\n",
    "<br>\n",
    "\n",
    "**The first part of the problem is to identify which parts of two images capture the same physical points of a scene, such as the corners of a window. This is typically achieved with local features (key locations in an image that can be reliably identified across different views). Local features contain short description vectors that capture the appearance around the point of interest. By comparing these descriptors, likely correspondences can be established between the pixel coordinates of image locations across two or more images. This ‚Äúimage registration‚Äù makes it possible to recover the 3D location of the point by triangulation.**\n",
    "\n",
    "<br>\n",
    "\n",
    "Google employs Structure-from-Motion techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic, and better leverage the volume of data already publicly available, Google presents this competition in collaboration with the University of British Columbia and Czech Technical University.\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/image3.gif\"></center>\n",
    "\n",
    "In this code competition, you‚Äôll create a machine learning algorithm that registers two images from different viewpoints. With access to a dataset of thousands of images to train and test your model, top-scoring notebooks will do so with the most accuracy.\n",
    "\n",
    "If successful, you'll help solve this well-known problem in computer vision, making it possible to map the world with unstructured image collections. Your solutions will have applications in photography and cultural heritage preservation, along with Google Maps. Winners will also be invited to give a presentation as part of the Image Matching: Local Features and Beyond workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) in June."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b741c4",
   "metadata": {
    "papermill": {
     "duration": 0.04023,
     "end_time": "2022-04-25T21:34:26.160983",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.120753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 COMPETITION EVALUATION\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n",
    "\n",
    "Participants are asked to estimate the relative pose of one image with respect to another. \n",
    "* Submissions are evaluated on the **m**ean **A**verage **A**ccuracy (**mAA**) of the estimated poses. \n",
    "* Given a fundamental matrix and the hidden ground truth, we compute the error in terms of rotation and translation:\n",
    "    * **rotation** --> **ùúñùëÖ** (in degrees) \n",
    "    * **translation** --> **ùúñùëá** (in meters)\n",
    "* Given one threshold over each, we classify a pose as accurate if it meets both thresholds. We do this over ten pairs of thresholds, one pair at a time\n",
    "    * i.e. at 1<sup>ùëú</sup> and 20 cm for the finest level\n",
    "    * i.e. at 10<sup>ùëú</sup> and 5 m for the coarsest level\n",
    "```python\n",
    "thresholds_r = np.linspace(1, 10, 10)  # In degrees.\n",
    "thresholds_t = np.geomspace(0.2, 5, 10)  # In meters.\n",
    "```\n",
    "* We then calculate the percentage of image pairs that meet every pair of thresholds, and average the results over all thresholds, which rewards more accurate poses. \n",
    "* As the dataset contains multiple scenes, which may have a different number of pairs, **we compute this metric separately for each scene and average it afterwards**. \n",
    "* A python implementation of this metric is available on <a href=\"https://www.kaggle.com/code/eduardtrulls/imc2022-tutorial-load-and-evaluate-training-data\">this notebook</a>.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FILE</b>\n",
    "\n",
    "For each ID in the test set, you must predict the fundamental matrix between the two views. The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "sample_id,fundamental_matrix\n",
    "a;b;c-d,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\n",
    "a;b;e-f,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\n",
    "a;b;g-h,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\n",
    "etc\n",
    "```\n",
    "\n",
    "Note that **`fundamental_matrix`** is a **`3√ó3`** matrix, flattened into a vector in row-major order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90cdcb",
   "metadata": {
    "papermill": {
     "duration": 0.045727,
     "end_time": "2022-04-25T21:34:26.244554",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.198827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.3 OTHER NOTABLE THINGS\n",
    "---\n",
    "\n",
    "This competition is part of the <a href=\"https://image-matching-workshop.github.io/\">Image Matching: Local Features and Beyond</a> workshop at <a href=\"https://cvpr2022.thecvf.com/\">CVPR'22</a>. Selected submissions to the competition will be invited to give talks at the workshop on June 20, 2022 in New Orleans, USA. Attending the workshop is not required to participate in the competition, but only teams that are attending the workshop will be considered to present their work.\n",
    "\n",
    "CVPR 2022 will be a hybrid conference. Attendees presenting in person are responsible for all costs associated with expenses and fees to attend CVPR 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c385e",
   "metadata": {
    "papermill": {
     "duration": 0.037282,
     "end_time": "2022-04-25T21:34:26.319659",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.282377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.4 DATASET OVERVIEW\n",
    "\n",
    "---\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n",
    "\n",
    "Aligning photographs of the same scene is a problem of longstanding interest to computer vision researchers. Your challenge in this competition is to generate mappings between pairs of photos from various cities.\n",
    "\n",
    "This competition uses a hidden test (n~=10_000). \n",
    "* When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook.\n",
    "\n",
    "<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">FILE INFORMATION</b>\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">train/*/calibration.csv</code>\n",
    "* **`image_id`**\n",
    "    * The image filename.\n",
    "* **`camera_intrinsics`** \n",
    "    * The 3√ó3 calibration matrix ùêä for this image, flattened into a vector by row-major indexing.\n",
    "* **`rotation_matrix`**\n",
    "    * The 3√ó3 rotation matrix ùêë for this image, flattened into a vector by row-major indexing.\n",
    "* **`translation_vector`**\n",
    "    * The translation vector ùêì.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">train/*/pair_covisibility.csv</code>\n",
    "* **`pair`**\n",
    "    * A string identifying a pair of images, encoded as two image filenames (without the extension) separated by a hyphen, as **`key1-key2`**, where **`key1 > key2`**.\n",
    "* **`covisibility`**\n",
    "    * An estimate of the overlap between the two images. \n",
    "    * Higher numbers indicate greater overlap. \n",
    "    * We recommend using all pairs with a covisibility estimate of 0.1 or above. \n",
    "    * The procedure used to derive this number is described in Section 3.2 and Figure 5 of <a href=\"https://arxiv.org/pdf/2003.01587.pdf\">this paper</a>.\n",
    "* **`fundamental_matrix1`**\n",
    "    * **The target column** as derived from the calibration files. \n",
    "    * Please see the <a href=\"https://www.kaggle.com/competitions/image-matching-challenge-2022/overview/problem-definition\">problem definition page</a> for more details.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">train/scaling_factors.csv</code> \n",
    "* The poses for each scene where reconstructed via <a href=\"https://en.wikipedia.org/wiki/Structure_from_motion\">Structure-from-Motion</a>, and are only accurate up to a scaling factor. \n",
    "* This file contains a scalar for each scene which can be used to convert them to meters. \n",
    "* For code examples, please refer to <a href=\"https://www.kaggle.com/eduardtrulls/imc2022-tutorial-load-and-evaluate-training-data\">this notebook</a>.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">train/*/images/</code>\n",
    "* A batch of images all taken near the same location.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">train/LICENSE.txt</code>\n",
    "* Records of the specific source of and license for each image.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">sample_submission.csv</code>\n",
    "* A valid sample submission.\n",
    "* **`sample_id`**\n",
    "    * The unique identifier for the image pair.\n",
    "* **`fundamental_matrix`**\n",
    "    * The target column. Please see the <a href=\"https://www.kaggle.com/competitions/image-matching-challenge-2022/overview/problem-definition\">problem definition page</a> for more details. \n",
    "    * The default values are randomly generated.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">test.csv</code> \n",
    "* Expect to see roughly 10,000 pairs of images in the hidden test set.\n",
    "* **`sample_id`**\n",
    "    * The unique identifier for the image pair.\n",
    "* **`batch_id`**\n",
    "    * The batch ID.\n",
    "* **`image_[1/2]_id`**\n",
    "    * The filenames of each image in the pair.\n",
    "\n",
    "<code style=\"font-weight: bold; font-size: 14px;\">test_images</code> \n",
    "The test set. \n",
    "* The test data comes from a different source than the train data and contains photos of mostly urban scenes with variable degrees of overlap. \n",
    "* The two images forming a pair may have been collected months or years apart, but **never less than 24 hours.**\n",
    "* Bridging this domain gap is part of the competition. \n",
    "* The images have been resized so that the **longest edge is around 800 pixels**, **may have different aspect ratios (including portrait and landscape), and are upright**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f8098",
   "metadata": {
    "papermill": {
     "duration": 0.040797,
     "end_time": "2022-04-25T21:34:26.397837",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.357040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. SETUP\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d3cb8",
   "metadata": {
    "papermill": {
     "duration": 0.037171,
     "end_time": "2022-04-25T21:34:26.472204",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.435033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 ACCELERATOR DETECTION\n",
    "---\n",
    "\n",
    "In order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n",
    "\n",
    "1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n",
    "2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# The name you gave to the TPU to use\n",
    "TPU_WORKER = 'my-tpu-name'\n",
    "\n",
    "# or you can also specify the grpc path directly\n",
    "# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n",
    "\n",
    "# The zone you chose when you created the TPU to use on GCP.\n",
    "ZONE = 'us-east1-b'\n",
    "\n",
    "# The name of the GCP project where you created the TPU to use on GCP.\n",
    "PROJECT = 'my-tpu-project'\n",
    "\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n",
    "```\n",
    "\n",
    "**WARNING:**\n",
    "\n",
    "\n",
    "Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.\n",
    "\n",
    "**REFERENCES:**\n",
    "\n",
    "\n",
    "- [ Guide - Use TPUs](https://www.tensorflow.org/guide/tpu#tpu_initialization)\n",
    "- [Doc - TPUClusterResolver](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77df475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:26.553715Z",
     "iopub.status.busy": "2022-04-25T21:34:26.552982Z",
     "iopub.status.idle": "2022-04-25T21:34:26.563612Z",
     "shell.execute_reply": "2022-04-25T21:34:26.563110Z",
     "shell.execute_reply.started": "2022-04-14T22:28:47.919746Z"
    },
    "papermill": {
     "duration": 0.054474,
     "end_time": "2022-04-25T21:34:26.563764",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.509290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n",
    "\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
    "except ValueError:\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "else:\n",
    "    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n",
    "    # Yield the default distribution strategy in Tensorflow\n",
    "    #   --> Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "\n",
    "# What Is a Replica?\n",
    "#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n",
    "#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n",
    "#    --> Each replica is essentially a copy of the training graph that is run on each core and \n",
    "#        trains a mini-batch containing 1/8th of the overall batch size\n",
    "N_REPLICAS = strategy.num_replicas_in_sync\n",
    "    \n",
    "print(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n",
    "\n",
    "print(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8169c1",
   "metadata": {
    "papermill": {
     "duration": 0.037563,
     "end_time": "2022-04-25T21:34:26.639553",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.601990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 COMPETITION DATA ACCESS\n",
    "---\n",
    "\n",
    "TPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n",
    "\n",
    "**TIPS:**\n",
    "\n",
    "\n",
    "If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07bc71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:26.722094Z",
     "iopub.status.busy": "2022-04-25T21:34:26.721509Z",
     "iopub.status.idle": "2022-04-25T21:34:26.730187Z",
     "shell.execute_reply": "2022-04-25T21:34:26.729750Z",
     "shell.execute_reply.started": "2022-04-14T22:28:47.94293Z"
    },
    "papermill": {
     "duration": 0.053045,
     "end_time": "2022-04-25T21:34:26.730295",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.677250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n",
    "\n",
    "if TPU:\n",
    "    # Google Cloud Dataset path to training and validation images\n",
    "    DATA_DIR = KaggleDatasets().get_gcs_path('image-matching-challenge-2022')\n",
    "    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "else:\n",
    "    # Local path to training and validation images\n",
    "    DATA_DIR = \"/kaggle/input/image-matching-challenge-2022\"\n",
    "    save_locally = None\n",
    "    load_locally = None\n",
    "\n",
    "print(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n",
    "\n",
    "print(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\n",
    "for file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n",
    "\n",
    "print(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb54233",
   "metadata": {
    "papermill": {
     "duration": 0.03945,
     "end_time": "2022-04-25T21:34:26.807637",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.768187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 LEVERAGING XLA OPTIMIZATIONS\n",
    "\n",
    "---\n",
    "**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n",
    "\n",
    "<br>\n",
    "\n",
    "When a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n",
    "\n",
    "XLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n",
    "\n",
    "\n",
    "**WARNING:**\n",
    "\n",
    "XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n",
    "\n",
    "**REFERENCE:**\n",
    "\n",
    "- [XLA: Optimizing Compiler for Machine Learning](https://www.tensorflow.org/xla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95897e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:26.890123Z",
     "iopub.status.busy": "2022-04-25T21:34:26.889526Z",
     "iopub.status.idle": "2022-04-25T21:34:26.892233Z",
     "shell.execute_reply": "2022-04-25T21:34:26.892687Z",
     "shell.execute_reply.started": "2022-04-14T22:28:47.964174Z"
    },
    "papermill": {
     "duration": 0.046885,
     "end_time": "2022-04-25T21:34:26.892821",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.845936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n",
    "\n",
    "print(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n",
    "# enable XLA optmizations (10% speedup when using @tf.function calls)\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "print(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c32a6",
   "metadata": {
    "papermill": {
     "duration": 0.037841,
     "end_time": "2022-04-25T21:34:26.968688",
     "exception": false,
     "start_time": "2022-04-25T21:34:26.930847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf131da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:27.059968Z",
     "iopub.status.busy": "2022-04-25T21:34:27.059316Z",
     "iopub.status.idle": "2022-04-25T21:34:42.759321Z",
     "shell.execute_reply": "2022-04-25T21:34:42.759726Z",
     "shell.execute_reply.started": "2022-04-14T22:28:47.974983Z"
    },
    "papermill": {
     "duration": 15.752551,
     "end_time": "2022-04-25T21:34:42.759880",
     "exception": false,
     "start_time": "2022-04-25T21:34:27.007329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n",
    "\n",
    "print(\"\\n... TRAIN META ...\\n\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "SCALING_CSV = os.path.join(TRAIN_DIR, \"scaling_factors.csv\")\n",
    "scaling_df = pd.read_csv(SCALING_CSV)\n",
    "scale_map = scaling_df.groupby(\"scene\")[\"scaling_factor\"].first().to_dict()\n",
    "\n",
    "# 'british_museum', 'piazza_san_marco', \n",
    "# 'trevi_fountain', 'st_pauls_cathedral', \n",
    "# 'colosseum_exterior', 'buckingham_palace', \n",
    "# 'temple_nara_japan', 'sagrada_familia', \n",
    "# 'grand_place_brussels', 'pantheon_exterior', \n",
    "# 'notre_dame_front_facade', 'st_peters_square', \n",
    "# 'sacre_coeur', 'taj_mahal', \n",
    "# 'lincoln_memorial_statue', 'brandenburg_gate'\n",
    "TRAIN_SCENES = scaling_df.scene.unique().tolist()\n",
    "\n",
    "train_map = {} \n",
    "for _s in tqdm(TRAIN_SCENES, total=len(TRAIN_SCENES)):\n",
    "    # Initialize    \n",
    "    train_map[_s] = {}\n",
    "    \n",
    "    # Image Stuff\n",
    "    train_map[_s][\"images\"] = sorted(tf.io.gfile.glob(os.path.join(TRAIN_DIR, _s, \"images\", \"*.jpg\")))\n",
    "    train_map[_s][\"image_ids\"] = [_f_path[:-4].rsplit(\"/\", 1)[-1] for _f_path in train_map[_s][\"images\"]]\n",
    "    \n",
    "    # Calibration Stuff (CAL)\n",
    "    _tmp_cal_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"calibration.csv\"))\n",
    "    _tmp_cal_df[\"image_path\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_cal_df[\"image_id\"]+\".jpg\"\n",
    "    train_map[_s][\"cal_df\"]=_tmp_cal_df.copy()\n",
    "        \n",
    "    # Pair Covisibility Stuff (PCO)\n",
    "    _tmp_pco_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"pair_covisibility.csv\"))\n",
    "    _tmp_pco_df[\"image_id_1\"], _tmp_pco_df[\"image_id_2\"] = zip(*_tmp_pco_df.pair.apply(lambda x: x.split(\"-\")))\n",
    "    _tmp_pco_df[\"image_path_1\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_1\"]+\".jpg\"\n",
    "    _tmp_pco_df[\"image_path_2\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_2\"]+\".jpg\"\n",
    "    train_map[_s][\"pco_df\"] = _tmp_pco_df.copy()\n",
    "\n",
    "#cleanup\n",
    "del _tmp_cal_df, _tmp_pco_df; gc.collect(); gc.collect();\n",
    "    \n",
    "print(\"\\n... TEST META ...\\n\")\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, \"test_images\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "test_df[\"f_path_1\"] = TEST_IMG_DIR+\"/\"+test_df.batch_id+\"/\"+test_df.image_1_id+\".png\"\n",
    "test_df[\"f_path_2\"] = TEST_IMG_DIR+\"/\"+test_df.batch_id+\"/\"+test_df.image_2_id+\".png\"\n",
    "display(test_df)\n",
    "\n",
    "SS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "ss_df = pd.read_csv(SS_CSV)\n",
    "display(ss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae2ad6",
   "metadata": {
    "papermill": {
     "duration": 0.05988,
     "end_time": "2022-04-25T21:34:42.866178",
     "exception": false,
     "start_time": "2022-04-25T21:34:42.806298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. HELPER FUNCTION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80866eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:42.994640Z",
     "iopub.status.busy": "2022-04-25T21:34:42.993548Z",
     "iopub.status.idle": "2022-04-25T21:34:43.461419Z",
     "shell.execute_reply": "2022-04-25T21:34:43.460513Z",
     "shell.execute_reply.started": "2022-04-14T22:29:06.87666Z"
    },
    "papermill": {
     "duration": 0.549681,
     "end_time": "2022-04-25T21:34:43.461566",
     "exception": false,
     "start_time": "2022-04-25T21:34:42.911885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_l_o_l(nested_list):\n",
    "    \"\"\" Flatten a list of lists \"\"\"\n",
    "    nested_list = [x if type(x) is list else [x,] for x in nested_list]\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def plot_two_paths(f_path_1, f_path_2, _titles=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    if _titles is None:\n",
    "        plt.title(f_path_1, fontweight=\"bold\")\n",
    "    else:\n",
    "        plt.title(_titles[0], fontweight=\"bold\")\n",
    "    plt.imshow(cv2.imread(f_path_1)[..., ::-1])\n",
    "    plt.axis(False)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    if _titles is None:\n",
    "        plt.title(f_path_2, fontweight=\"bold\")\n",
    "    else:\n",
    "        plt.title(_titles[1], fontweight=\"bold\")\n",
    "\n",
    "    plt.imshow(cv2.imread(f_path_2)[..., ::-1])\n",
    "    plt.axis(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def pco_random_plot(_df):\n",
    "    df_row = _df.sample(1).reset_index(drop=True)\n",
    "    plot_two_paths(\n",
    "        df_row.image_path_1[0],\n",
    "        df_row.image_path_2[0],\n",
    "        [f\"Image ID: {df_row.image_id_1} - COV={df_row.covisibility[0]}\",\n",
    "         f\"Image ID: {df_row.image_id_2} - COV={df_row.covisibility[0]}\",]\n",
    "    )\n",
    "    \n",
    "def arr_from_str(_s):\n",
    "    return np.fromstring(_s, sep=\" \").reshape((-1,3)).squeeze()\n",
    "\n",
    "def get_id2cal_map(_train_map):\n",
    "    _id2cal_map = {}\n",
    "    for _s in tqdm(TRAIN_SCENES, total=len(TRAIN_SCENES)):\n",
    "        for _, _row in _train_map[_s][\"cal_df\"].iterrows():\n",
    "            _img_id = _row[\"image_id\"]; _id2cal_map[_img_id]={}\n",
    "            _id2cal_map[_img_id][\"camera_intrinsics\"] = arr_from_str(_row[\"camera_intrinsics\"])\n",
    "            _id2cal_map[_img_id][\"rotation_matrix\"] = arr_from_str(_row[\"rotation_matrix\"])\n",
    "            _id2cal_map[_img_id][\"translation_vector\"] = arr_from_str(_row[\"translation_vector\"])\n",
    "            _id2cal_map[_img_id][\"image_path\"] = _row[\"image_path\"]\n",
    "    return _id2cal_map\n",
    "\n",
    "id2cal_map = get_id2cal_map(train_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61800d6c",
   "metadata": {
    "papermill": {
     "duration": 0.041005,
     "end_time": "2022-04-25T21:34:43.544185",
     "exception": false,
     "start_time": "2022-04-25T21:34:43.503180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The following functions are for feature identification, matrix manipulations, etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26aedec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:43.662151Z",
     "iopub.status.busy": "2022-04-25T21:34:43.634269Z",
     "iopub.status.idle": "2022-04-25T21:34:43.664002Z",
     "shell.execute_reply": "2022-04-25T21:34:43.664360Z",
     "shell.execute_reply.started": "2022-04-14T22:29:07.624328Z"
    },
    "papermill": {
     "duration": 0.079395,
     "end_time": "2022-04-25T21:34:43.664543",
     "exception": false,
     "start_time": "2022-04-25T21:34:43.585148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_sift_features(rgb_image, detector, n_features):\n",
    "    \"\"\" \n",
    "    Helper Function to Compute SIFT features for a Given Image\n",
    "    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n",
    "    \"\"\"\n",
    "    # Convert RGB image to Grayscale\n",
    "    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Run detector and retrieve only the top keypoints and descriptors \n",
    "    kp, desc = detector.detectAndCompute(gray, None)\n",
    "    \n",
    "    return kp[:n_features], desc[:n_features]\n",
    "\n",
    "def extract_keypoints(rgb_image, detector):\n",
    "    \"\"\" \n",
    "    Helper Function to Compute SIFT features for a Given Image\n",
    "    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n",
    "    \"\"\"\n",
    "    # Convert RGB image to Grayscale\n",
    "    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Run detector and retrieve the keypoints\n",
    "    return detector.detect(gray)\n",
    "\n",
    "def build_composite_image(im1, im2, axis=1, margin=0, background=1):\n",
    "    \"\"\"\n",
    "    Convenience function to stack two images with different sizes.\n",
    "    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n",
    "    \"\"\"\n",
    "    \n",
    "    if background != 0 and background != 1:\n",
    "        background = 1\n",
    "    if axis != 0 and axis != 1:\n",
    "        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n",
    "\n",
    "    h1, w1, _ = im1.shape\n",
    "    h2, w2, _ = im2.shape\n",
    "\n",
    "    if axis == 1:\n",
    "        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n",
    "        if h1 > h2:\n",
    "            voff1, voff2 = 0, (h1 - h2) // 2\n",
    "        else:\n",
    "            voff1, voff2 = (h2 - h1) // 2, 0\n",
    "        hoff1, hoff2 = 0, w1 + margin\n",
    "    else:\n",
    "        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n",
    "        if w1 > w2:\n",
    "            hoff1, hoff2 = 0, (w1 - w2) // 2\n",
    "        else:\n",
    "            hoff1, hoff2 = (w2 - w1) // 2, 0\n",
    "        voff1, voff2 = 0, h1 + margin\n",
    "    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n",
    "    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n",
    "\n",
    "    return (composite, (voff1, voff2), (hoff1, hoff2))\n",
    "\n",
    "\n",
    "def draw_cv_matches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n",
    "    \"\"\"\n",
    "    Draw keypoints and matches.\n",
    "    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n",
    "    \"\"\"\n",
    "    \n",
    "    composite, v_offset, h_offset = build_composite_image(im1, im2, axis, margin, background)\n",
    "\n",
    "    # Draw all keypoints.\n",
    "    for coord_a, coord_b in zip(kp1, kp2):\n",
    "        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "    \n",
    "    # Draw matches, and highlight keypoints used in matches.\n",
    "    for idx_a, idx_b in matches:\n",
    "        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n",
    "        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n",
    "        composite = cv2.line(composite,\n",
    "                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n",
    "                                   int(kp1[idx_a][1] + v_offset[0])]),\n",
    "                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n",
    "                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n",
    "    return composite\n",
    "\n",
    "def normalize_keypoints(keypoints, K):\n",
    "    C_x = K[0, 2]\n",
    "    C_y = K[1, 2]\n",
    "    f_x = K[0, 0]\n",
    "    f_y = K[1, 1]\n",
    "    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n",
    "    return keypoints\n",
    "\n",
    "def compute_essential_matrix(F, K1, K2, kp1, kp2):\n",
    "    \"\"\"\n",
    "    Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. \n",
    "    Note that we ask participants to estimate F, i.e., without relying on known intrinsics.\n",
    "    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n",
    "    \"\"\"\n",
    "    \n",
    "    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n",
    "    # We do not account for this here, as the modern RANSACs do not do this:\n",
    "    # https://opencv.org/evaluating-opencvs-new-ransacs\n",
    "    assert F.shape[0] == 3, 'Malformed F?'\n",
    "\n",
    "    # Use OpenCV's recoverPose to solve the cheirality check:\n",
    "    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n",
    "    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n",
    "    \n",
    "    kp1n = normalize_keypoints(kp1, K1)\n",
    "    kp2n = normalize_keypoints(kp2, K2)\n",
    "    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n",
    "\n",
    "    return E, R, T\n",
    "\n",
    "def quaternion_from_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Transform a rotation matrix into a quaternion\n",
    "    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n",
    "    \n",
    "    The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\\\n",
    "    \"\"\"\n",
    "\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n",
    "    m00 = M[0, 0]\n",
    "    m01 = M[0, 1]\n",
    "    m02 = M[0, 2]\n",
    "    m10 = M[1, 0]\n",
    "    m11 = M[1, 1]\n",
    "    m12 = M[1, 2]\n",
    "    m20 = M[2, 0]\n",
    "    m21 = M[2, 1]\n",
    "    m22 = M[2, 2]\n",
    "\n",
    "    K = np.array([\n",
    "        [m00 - m11 - m22, 0.0, 0.0, 0.0],\n",
    "        [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n",
    "        [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n",
    "        [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]\n",
    "    ])\n",
    "    K /= 3.0\n",
    "\n",
    "    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n",
    "    w, V = np.linalg.eigh(K)\n",
    "    q = V[[3, 0, 1, 2], np.argmax(w)]\n",
    "    if q[0] < 0:\n",
    "        np.negative(q, q)\n",
    "    return q\n",
    "\n",
    "\n",
    "def compute_error_for_example(q_gt, T_gt, q, T, scale, eps=1e-15):\n",
    "    '''Compute the error metric for a single example.\n",
    "    \n",
    "    The function returns two errors, over rotation and translation. \n",
    "    These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n",
    "    \n",
    "    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n",
    "    q_norm = q / (np.linalg.norm(q) + eps)\n",
    "\n",
    "    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n",
    "    err_q = np.arccos(1 - 2 * loss_q)\n",
    "\n",
    "    # Apply the scaling factor for this scene.\n",
    "    T_gt_scaled = T_gt * scale\n",
    "    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n",
    "\n",
    "    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n",
    "\n",
    "    return err_q * 180 / np.pi, err_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbfbac",
   "metadata": {
    "papermill": {
     "duration": 0.041135,
     "end_time": "2022-04-25T21:34:43.747167",
     "exception": false,
     "start_time": "2022-04-25T21:34:43.706032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. DATASET EXPLORATION & PREPROCESSING\n",
    "---\n",
    "\n",
    "Let's investigate the provided information and then plot some images for various things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7eb48",
   "metadata": {
    "papermill": {
     "duration": 0.049157,
     "end_time": "2022-04-25T21:34:43.839393",
     "exception": false,
     "start_time": "2022-04-25T21:34:43.790236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1 THE TRAINING DATA\n",
    "---\n",
    "\n",
    "Before we can make some simple deductions, we need to combine the data as much as possible so we can view it with a comprehensive lense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620ef01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:43.955513Z",
     "iopub.status.busy": "2022-04-25T21:34:43.954833Z",
     "iopub.status.idle": "2022-04-25T21:34:46.302288Z",
     "shell.execute_reply": "2022-04-25T21:34:46.302708Z",
     "shell.execute_reply.started": "2022-04-14T22:29:07.672817Z"
    },
    "papermill": {
     "duration": 2.415692,
     "end_time": "2022-04-25T21:34:46.302868",
     "exception": false,
     "start_time": "2022-04-25T21:34:43.887176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_cal_dfs = []\n",
    "_pco_dfs = []\n",
    "_s_dfs = []\n",
    "for _s in TRAIN_SCENES:\n",
    "    _s_map = train_map[_s]\n",
    "    _s_dfs.append(pd.DataFrame({\n",
    "        \"scene\":[_s,]*len(_s_map[\"images\"]),\n",
    "        \"f_path\":_s_map[\"images\"],\n",
    "    }))\n",
    "    _cal_dfs.append(_s_map[\"cal_df\"])\n",
    "    _pco_dfs.append(_s_map[\"pco_df\"])\n",
    "\n",
    "all_pco_dfs = pd.concat(_pco_dfs).reset_index(drop=True)\n",
    "all_cal_dfs = pd.concat(_cal_dfs).reset_index(drop=True)\n",
    "\n",
    "train_df = pd.concat(_s_dfs).reset_index(drop=True)\n",
    "train_df.insert(0, \"image_id\", train_df.f_path.apply(lambda x: x[:-4].rsplit(\"/\", 1)[-1]))\n",
    "train_df = pd.merge(train_df, all_cal_dfs, on=\"image_id\").drop(columns=[\"image_path\",])\n",
    "\n",
    "cov_1_df = all_pco_dfs[[\"image_id_1\", \"covisibility\"]]\n",
    "cov_1_df.columns = [\"image_id\", \"covisibility\"]\n",
    "cov_2_df = all_pco_dfs[[\"image_id_2\", \"covisibility\"]]\n",
    "cov_2_df.columns = [\"image_id\", \"covisibility\"]\n",
    "cov_df = pd.concat([cov_1_df,cov_2_df])\n",
    "img_id_2_cov = cov_df.groupby(\"image_id\")[\"covisibility\"].mean().to_dict()\n",
    "del cov_1_df, cov_2_df, cov_df; gc.collect();\n",
    "\n",
    "# Add a column for average covisibility\n",
    "#    - i.e. For a given image, what is the average \n",
    "#           covisibility of all of it's respective pairs\n",
    "train_df.insert(2, \"mean_covisibility\", train_df.image_id.map(img_id_2_cov))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f8047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:46.392805Z",
     "iopub.status.busy": "2022-04-25T21:34:46.392214Z",
     "iopub.status.idle": "2022-04-25T21:34:47.414253Z",
     "shell.execute_reply": "2022-04-25T21:34:47.414799Z",
     "shell.execute_reply.started": "2022-04-14T22:29:10.391979Z"
    },
    "papermill": {
     "duration": 1.069919,
     "end_time": "2022-04-25T21:34:47.414967",
     "exception": false,
     "start_time": "2022-04-25T21:34:46.345048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(train_df, \"scene\", color=\"scene\", title=\"<b>Image Counts By Scene</b>\")\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images</b>\", xaxis_title=\"<b>Scene Identifier</b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\", showlegend=False,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(train_df, \"mean_covisibility\", color=\"scene\", title=\"<b>Average Covisibility Of Images Coloured By Scene</b>\", )\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Covisibility Bin</b>\", xaxis_title=\"<b>Covisibility (0-1.00)</b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20751f2",
   "metadata": {
    "papermill": {
     "duration": 0.051133,
     "end_time": "2022-04-25T21:34:47.518797",
     "exception": false,
     "start_time": "2022-04-25T21:34:47.467664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2 LET'S LOOK INTO THE CAMERA MATRIX\n",
    "---\n",
    "\n",
    "- [REFERENCE](https://www.wikiwand.com/en/Camera_matrix) \n",
    "The calibration matrices for a given camera calculate the camera matrix using the extrinsic and intrinsic parameters. The extrinsic parameters represent a rigid transformation from 3-D world coordinate system to the 3-D camera‚Äôs coordinate system. The intrinsic parameters represent a projective transformation from the 3-D camera‚Äôs coordinates into the 2-D image coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "This is a fancy way of saying that **extrinsics** \n",
    "\n",
    "* describe the cameras location within the external (outside-the-camera) world \n",
    "    * <b>$(x,y,z)$</b>\n",
    "* describe the cameras orientation with respect to the external (outside-the-camera) world \n",
    "    * <b>$(\\gamma, \\beta, \\alpha)$</b>\n",
    "* Note while these are the values that describe the absolute positions/rotations of the camera, it is often useful to represent them as a series of operations that would move a camera from a level position at the origin to whatever respective position the camera exists at. This is where the concepts of the **rotation** and **translation** matrix come from.\n",
    "    \n",
    "while **intrinsics**...\n",
    "\n",
    "* Describe how a point in the 3D world is mapped to the 2D image plane\n",
    "    * focal length\n",
    "    * skew/distortion/scale\n",
    "    * principal offsets\n",
    "    * etc.\n",
    "\n",
    "We can take this to the logical conclusion of building a matrix that can fully describe a camera (the union of these two matrices). This new matrix is called **the camera matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In computer vision a camera matrix or (camera) projection matrix is a $3\\times 4$ matrix which describes the mapping of a pinhole camera from 3D points in the world to 2D points in an image.\n",
    "\n",
    "Let $\\mathbf {x}$  be a representation of a 3D point in homogeneous coordinates (a 4-dimensional vector), and let  $\\mathbf{y}$  be a representation of the image of this point in the pinhole camera (a 3-dimensional vector). Then the following relation holds\n",
    "\n",
    "$$\n",
    " \\mathbf{y} \\sim \\mathbf{C} \\, \\mathbf{x} \n",
    "$$\n",
    "\n",
    "where  $\\mathbf{C}$  is the camera matrix and the $\\, \\sim$  sign implies that the left and right hand sides are equal up to a non-zero scalar multiplication. Since the camera matrix  $\\mathbf{C}$  is involved in the mapping between elements of two projective spaces, it too can be regarded as a projective element. \n",
    "\n",
    "**This means that  $\\mathbf{C}$  has only 11 degrees of freedom since any multiplication by a non-zero scalar results in an equivalent camera matrix.**\n",
    "\n",
    "---\n",
    "\n",
    "***I will spare you the derivation and normalization steps... if you're interested they are detailed in the wiki page linked in reference one above.***\n",
    "\n",
    "---\n",
    "\n",
    "Given the mapping produced by a normalized camera matrix (skipped), the resulting normalized image coordinates can be transformed by means of an arbitrary 2D homography. This **includes 2D translations and rotations** as well as **scaling** (**isotropic** and **anisotropic**) but also general 2D perspective transformations. \n",
    "\n",
    "Such a transformation can be represented as a $3\\times  3$  matrix  $\\mathbf {H}$  which maps the homogeneous normalized image coordinates  $\\mathbf{y}$  to the homogeneous transformed image coordinates ${\\mathbf  {y}}'$:\n",
    "\n",
    "$$\n",
    "{\\mathbf  {y}}'={\\mathbf  {H}}\\,{\\mathbf  {y}}\n",
    "$$\n",
    "\n",
    "Inserting the above expression for the normalized image coordinates in terms of the 3D coordinates gives\n",
    "\n",
    "$$\n",
    "{\\mathbf  {y}}'={\\mathbf  {H}}\\,{\\mathbf  {C}}_N\\,{\\mathbf  {x}}'\n",
    "$$\n",
    "\n",
    "This produces the most general form of camera matrix (Note we sometimes use $\\mathbf {K}$ instead of $\\mathbf {H}$)\n",
    "\n",
    "$$\n",
    "{\\mathbf  {C}}={\\mathbf  {H}}\\,{\\mathbf  {C}}_N\\,{\\mathbf  {x}}'={\\mathbf  {H} \\left( {\\mathbf {R}} \\, \\middle| \\, {\\mathbf {t}} \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2a1d3",
   "metadata": {
    "papermill": {
     "duration": 0.052272,
     "end_time": "2022-04-25T21:34:47.622560",
     "exception": false,
     "start_time": "2022-04-25T21:34:47.570288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.3 LET'S LOOK INTO THE CAMERA INSTRICS\n",
    "---\n",
    "- [REFERENCE](https://ksimek.github.io/2013/08/13/intrinsic)\n",
    "\n",
    "The intrinsic matrix is parameterized by Hartley and Zisserman as\n",
    "\n",
    "$$K=\\begin{pmatrix}\n",
    "f_x&s&x_0\\\\\n",
    "0&f_y&y_0\\\\\n",
    "0&0&1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Each intrinsic parameter describes a geometric property of the camera. \n",
    "\n",
    "Let's examine each of these properties in detail.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Focal Length** ‚Äì‚Äì $(f_x, f_y)$\n",
    "\n",
    "The focal length is the distance between the pinhole and the film (a.k.a. image plane). \n",
    "* For reasons... the focal length is measured in pixels. \n",
    "* In a true pinhole camera, both $f_x$ and $f_y$ have the same value, which is illustrated as $f$ below.\n",
    "    * In the code cell below we verify that for all our examples $f_x = f_y$\n",
    "* In practice, $f_x$ and $f_y$ can differ for a number of reasons... however, we will ignore them for now as in our dataset... these two values are always equal (assume perfectly square pixels!)\n",
    "\n",
    "<center><img src=\"https://ksimek.github.io/img/intrinsic-focal-length.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Principal Point Offset** ‚Äì‚Äì $(x_0, y_0)$\n",
    "\n",
    "The camera's **\"principal axis\"** is the line perpendicular to the image plane that passes through the pinhole. \n",
    "* Its itersection with the image plane is referred to as the **\"principal point\"**, illustrated below.\n",
    "\n",
    "<center><img src=\"https://ksimek.github.io/img/intrinsic-pp.png\"></center>\n",
    "\n",
    "The **\"principal point offset\"** is the location of the principal point relative to **the film's origin**. \n",
    "* The exact definition depends on which convention is used for the location of the origin\n",
    "* the illustration below assumes it's at the bottom-left of the film.\n",
    "\n",
    "<center><img src=\"https://ksimek.github.io/img/intrinsic-pp-offset.png\"></center>\n",
    "\n",
    "Increasing $x_0$ shifts the pinhole to the right\n",
    "\n",
    "<center><img src=\"https://ksimek.github.io/img/intrinsic-pp-offset-delta-alt.png\"></center>\n",
    "\n",
    "This is equivalent to shifting the film to the left and leaving the pinhole unchanged.\n",
    "\n",
    "<center><img src=\"https://ksimek.github.io/img/intrinsic-pp-offset-delta.png\"></center>\n",
    "\n",
    "Notice that the box surrounding the camera is irrelevant, only the pinhole's position relative to the film matters.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Axis Skew** ‚Äì‚Äì $(s)$\n",
    "\n",
    "Axis skew causes shear distortion in the projected image. \n",
    "* In our dataset all skew values should be 0... but it's worth noting that some digitization processes can cause nonzero skew\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "We can decompose the intrinsic matrix into a sequence of shear, scaling, and translation transformations, corresponding to axis skew, focal length, and principal point offset, respectively:\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/Gvt4CBx/Screen-Shot-2022-04-11-at-1-38-20-PM.png\"></center>\n",
    "\n",
    "---\n",
    "\n",
    "Now that we understand the basics of the camera intrinsic matrix, we can investigate the distribution of the respective values within the provided train dataset. We will first extract the respective information entities from the intrinsics matrix, verify certain assumptions, and then plot the relevant distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e184d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:47.769251Z",
     "iopub.status.busy": "2022-04-25T21:34:47.768634Z",
     "iopub.status.idle": "2022-04-25T21:34:47.773977Z",
     "shell.execute_reply": "2022-04-25T21:34:47.773527Z",
     "shell.execute_reply.started": "2022-04-14T22:29:11.639872Z"
    },
    "papermill": {
     "duration": 0.099706,
     "end_time": "2022-04-25T21:34:47.774106",
     "exception": false,
     "start_time": "2022-04-25T21:34:47.674400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: Our intrinsics matrix is a flat string... NOTE: [idx]\n",
    "#       but when reshaped to 3x3 it will map as follows...\n",
    "# STRING --> \"TL[0] T[1] TR[2] CL[3] C[4] CR[5] BL[6] B[7] BR[8]\"\n",
    "#\n",
    "# MATRIX BELOW\n",
    "##############\n",
    "#            #\n",
    "#  TL  T  TR #\n",
    "#  CL  C  CR #\n",
    "#  BL  B  BR #\n",
    "#            #\n",
    "##############\n",
    "# \n",
    "# Therefore we just index into the string and convert to a digit to access certain elements\n",
    "train_df[\"fx\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[0])) # 0 = TL \n",
    "train_df[\"fy\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[4])) # 4 = C\n",
    "train_df[\"x0\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[2])) # 0 = TR\n",
    "train_df[\"y0\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[5])) # 4 = CR\n",
    "train_df[\"s\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[1])) # 1 = TC\n",
    "\n",
    "if (train_df[\"fx\"]!=train_df[\"fy\"]).sum()==0:\n",
    "    print(\"All fx values (focal length x) are equal to the respective fy values (focal length y)... as expected\")\n",
    "    \n",
    "if train_df[\"s\"].sum()==0:\n",
    "    print(\"All skew values are 0... as expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467318f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:47.907053Z",
     "iopub.status.busy": "2022-04-25T21:34:47.906176Z",
     "iopub.status.idle": "2022-04-25T21:34:48.295772Z",
     "shell.execute_reply": "2022-04-25T21:34:48.296236Z",
     "shell.execute_reply.started": "2022-04-14T22:29:11.695856Z"
    },
    "papermill": {
     "duration": 0.465627,
     "end_time": "2022-04-25T21:34:48.296395",
     "exception": false,
     "start_time": "2022-04-25T21:34:47.830768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(train_df, \"x0\", title=\"<b>Principal Point Offset [x<sub>0</sub>] Coloured By Scene</b>\", color=\"scene\", log_y=False)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per x<sub>0</sub> Bin</b>\", xaxis_title=\"<b>Principal Point Offset [x<sub>0</sub>]</b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(train_df, \"y0\", title=\"<b>Principal Point Offset [y<sub>0</sub>] Coloured By Scene</b>\", color=\"scene\", log_y=False)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per y<sub>0</sub> Bin</b>\", xaxis_title=\"<b>Principal Point Offset [y<sub>0</sub>]</b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# We use \"fx\" because all \"fx\" values are equivalent to \"fy\" values\n",
    "fig = px.histogram(train_df, \"fx\", title=\"<b>Focal Lengths Coloured By Scene</b>\", color=\"scene\", log_x=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Focal Length Bin</b>\", xaxis_title=\"<b>Focal Length <i>(log scale)</i></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a28af",
   "metadata": {
    "papermill": {
     "duration": 0.056315,
     "end_time": "2022-04-25T21:34:48.415042",
     "exception": false,
     "start_time": "2022-04-25T21:34:48.358727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.4 LET'S LOOK INTO THE CAMERA EXTRINSICS\n",
    "---\n",
    "- [REFERENCE](https://ksimek.github.io/2012/08/22/extrinsic)\n",
    "\n",
    "As discussed previously, the camera's **extrinsic matrix** describes the **camera's location in the world (3D), and what direction it's pointing (orientation)**. It has two components: \n",
    "* **a rotation matrix, $\\mathbf{R}$ (or $\\theta$)**\n",
    "* **a translation vector $\\mathbf{t}$**\n",
    "\n",
    "However, shocker, these don't exactly correspond to the camera's rotation and translation. \n",
    "\n",
    "The extrinsic matrix takes the form of a rigid transformation matrix: \n",
    "* a $3\\times 3$ rotation matrix in the left-block\n",
    "* a $3\\times 1$ translation column-vector in the right\n",
    "\n",
    "$$\n",
    "\\left [\n",
    "    \\begin{array}{c|c}\n",
    "R & t\n",
    "    \\end{array}\n",
    "\\right] = \n",
    "\\left [\n",
    "    \\begin{array}{ccc|c}\n",
    "r_{1,1} & r_{1,2} & r_{1,3} & t_1 \\\\\n",
    "r_{2,1} & r_{2,2} & r_{2,3} & t_2 \\\\\n",
    "r_{3,1} & r_{3,2} & r_{3,3} & t_3\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "It's common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\left [\n",
    "        \\begin{array}{c|c} \n",
    "            R & \\boldsymbol{t} \\\\\n",
    "            \\hline\n",
    "            \\boldsymbol{0} & 1 \n",
    "        \\end{array}\n",
    "    \\right ] &= \n",
    "    \\left [\n",
    "        \\begin{array}{c|c} \n",
    "            I & \\boldsymbol{t} \\\\\n",
    "            \\hline\n",
    "            \\boldsymbol{0} & 1 \n",
    "        \\end{array}\n",
    "    \\right ] \n",
    "    \\times\n",
    "    \\left [\n",
    "        \\begin{array}{c|c} \n",
    "            R & \\boldsymbol{0} \\\\\n",
    "            \\hline\n",
    "            \\boldsymbol{0} & 1 \n",
    "        \\end{array}\n",
    "    \\right ] \\\\\n",
    "        &=\n",
    "\\left[ \\begin{array}{ccc|c} \n",
    "1 & 0 & 0 & t_1 \\\\\n",
    "0 & 1 & 0 & t_2 \\\\\n",
    "0 & 0 & 1 & t_3 \\\\\n",
    "  \\hline\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right] \\times\n",
    "\\left[ \\begin{array}{ccc|c} \n",
    "r_{1,1} & r_{1,2} & r_{1,3} & 0  \\\\\n",
    "r_{2,1} & r_{2,2} & r_{2,3} & 0 \\\\\n",
    "r_{3,1} & r_{3,2} & r_{3,3} & 0 \\\\\n",
    "  \\hline\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This matrix describes how to transform points in world coordinates to camera coordinates. \n",
    "* The vector $t$ can be interpreted as the position of the world origin in camera coordinates\n",
    "* The columns of $R$ represent represent the directions of the world-axes in camera coordinates.\n",
    "\n",
    "**The important thing to remember about the extrinsic matrix is that it describes how the world is transformed relative to the camera.** This is often counter-intuitive, because we usually want to specify how the camera is transformed relative to the world. \n",
    "\n",
    "---\n",
    "\n",
    "Let's briefly look into each matrix type to get a grasp of what is happening in each in laymens terms**\n",
    "\n",
    "In 3D space, rotation can occur about the x, y, or z-axis. Such a type of rotation that occurs about any one of the axis is known as a basic or elementary rotation. Given below are the rotation matrices that can rotate a vector through an angle about any particular axis.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_x = \\left [\n",
    "        \\begin{array}{ccc} \n",
    "            1 & 0 & 0 \\\\\n",
    "            0 & \\cos(\\boldsymbol{\\gamma}) & -\\sin(\\boldsymbol{\\gamma}) \\\\\n",
    "            0 & \\sin(\\boldsymbol{\\gamma}) & \\cos(\\boldsymbol{\\gamma}) \n",
    "        \\end{array}\n",
    "    \\right ] \n",
    "\\end{align} \\qquad\\text{This is also known as ROLL}\\quad\\text{(CC-rotation of }\\gamma\\text{ about the x-axis)}\n",
    "$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_y = \\left [\n",
    "        \\begin{array}{ccc} \n",
    "            \\cos(\\boldsymbol{\\beta}) & 0 & \\sin(\\boldsymbol{\\beta}) \\\\\n",
    "            0 & 1 & 0 \\\\\n",
    "            -\\sin(\\boldsymbol{\\beta}) & 0 & \\cos(\\boldsymbol{\\beta})\n",
    "        \\end{array}\n",
    "    \\right ]\n",
    "\\end{align} \\qquad\\text{This is also known as PITCH}\\quad\\text{(CC-rotation of }\\beta\\text{ about the y-axis)}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_z = \\left [\n",
    "        \\begin{array}{ccc} \n",
    "            \\cos(\\boldsymbol{\\alpha}) & -\\sin(\\boldsymbol{\\alpha}) & 0 \\\\\n",
    "            \\sin(\\boldsymbol{\\alpha}) & \\cos(\\boldsymbol{\\alpha}) & 0 \\\\\n",
    "            0 & 0 & 1 \n",
    "        \\end{array}\n",
    "    \\right ]\n",
    "\\end{align} \\qquad\\text{This is also known as YAW}\\quad\\text{(CC-rotation of }\\alpha\\text{ about the z-axis)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c323101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:48.634251Z",
     "iopub.status.busy": "2022-04-25T21:34:48.544857Z",
     "iopub.status.idle": "2022-04-25T21:34:50.109308Z",
     "shell.execute_reply": "2022-04-25T21:34:50.109826Z",
     "shell.execute_reply.started": "2022-04-15T00:14:35.386064Z"
    },
    "papermill": {
     "duration": 1.639734,
     "end_time": "2022-04-25T21:34:50.110013",
     "exception": false,
     "start_time": "2022-04-25T21:34:48.470279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"t_x\"] = train_df.translation_vector.apply(lambda x: float(x.split()[0]))\n",
    "train_df[\"t_y\"] = train_df.translation_vector.apply(lambda x: float(x.split()[1]))\n",
    "train_df[\"t_z\"] = train_df.translation_vector.apply(lambda x: float(x.split()[2]))\n",
    "\n",
    "train_df[\"r1_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[0]))\n",
    "train_df[\"r2_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[1]))\n",
    "train_df[\"r3_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[2]))\n",
    "train_df[\"r1_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[3]))\n",
    "train_df[\"r2_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[4]))\n",
    "train_df[\"r3_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[5]))\n",
    "train_df[\"r1_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[6]))\n",
    "train_df[\"r2_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[7]))\n",
    "train_df[\"r3_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[8]))\n",
    "\n",
    "fig = px.histogram(train_df, \"t_x\", title=\"<b>Translation Value <sub>x</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Translation Value <sub>x</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(train_df, \"t_y\", title=\"<b>Translation Value <sub>y</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Translation Value <sub>y</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(train_df, \"t_z\", title=\"<b>Translation Value <sub>z</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Translation Value <sub>z</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r1_1\", title=\"<b>Rotation Value <sub>1,1</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>1,1</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r2_1\", title=\"<b>Rotation Value <sub>2,1</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>2,1</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r3_1\", title=\"<b>Rotation Value <sub>3,1</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>3,1</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r1_2\", title=\"<b>Rotation Value <sub>1,2</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>1,2</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r2_2\", title=\"<b>Rotation Value <sub>2,2</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>2,2</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r3_2\", title=\"<b>Rotation Value <sub>3,2</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>3,2</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r1_3\", title=\"<b>Rotation Value <sub>1,3</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>1,3</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(train_df, \"r2_3\", title=\"<b>Rotation Value <sub>2,3</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>2,3</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(train_df, \"r3_3\", title=\"<b>Rotation Value <sub>3,3</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>3,3</sub></b>\",\n",
    "    legend_title=\"<b>Scene Identifier</b>\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2f0c0",
   "metadata": {
    "papermill": {
     "duration": 0.093626,
     "end_time": "2022-04-25T21:34:50.289746",
     "exception": false,
     "start_time": "2022-04-25T21:34:50.196120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.5 LOOK AT A SINGLE EXAMPLE W/ COVISIBILITY & CALIBRATION INFORMATION\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb2880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:50.472760Z",
     "iopub.status.busy": "2022-04-25T21:34:50.467814Z",
     "iopub.status.idle": "2022-04-25T21:34:52.332024Z",
     "shell.execute_reply": "2022-04-25T21:34:52.331537Z"
    },
    "papermill": {
     "duration": 1.957317,
     "end_time": "2022-04-25T21:34:52.332145",
     "exception": false,
     "start_time": "2022-04-25T21:34:50.374828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nEXAMPLE OF HIGH COVISIBILITY\")\n",
    "plot_two_paths(\n",
    "    train_map[\"british_museum\"][\"pco_df\"][\"image_path_1\"][0], \n",
    "    train_map[\"british_museum\"][\"pco_df\"][\"image_path_2\"][0],\n",
    "    [f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[0]}\",\n",
    "     f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[0]}\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP LEFT IMAGE (ID='93658023_4980549800') WE SEE THE FOLLOWING:\")\n",
    "cal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"93658023_4980549800\"]\n",
    "for _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n",
    "    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n",
    "    print(arr_from_str(cal_row_from_id[_property].values[0]))\n",
    "\n",
    "print(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP RIGHT IMAGE (ID='77723525_5227836172') WE SEE THE FOLLOWING:\")\n",
    "cal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"77723525_5227836172\"]\n",
    "for _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n",
    "    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n",
    "    print(arr_from_str(cal_row_from_id[_property].values[0]))\n",
    "\n",
    "    \n",
    "print(\"\\n\\n\\n\\nEXAMPLE OF NO COVISIBILITY\")\n",
    "plot_two_paths(\n",
    "    train_map[\"british_museum\"][\"pco_df\"][\"image_path_1\"][15395],\n",
    "    train_map[\"british_museum\"][\"pco_df\"][\"image_path_2\"][15395],\n",
    "    [f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[15395]}\",\n",
    "     f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[15395]}\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP LEFT IMAGE (ID='66757775_3535589713') WE SEE THE FOLLOWING:\")\n",
    "cal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"66757775_3535589713\"]\n",
    "for _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n",
    "    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n",
    "    print(arr_from_str(cal_row_from_id[_property].values[0]))\n",
    "\n",
    "print(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP RIGHT IMAGE (ID='66747696_4734591579') WE SEE THE FOLLOWING:\")\n",
    "cal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"66747696_4734591579\"]\n",
    "for _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n",
    "    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n",
    "    print(arr_from_str(cal_row_from_id[_property].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46308e",
   "metadata": {
    "papermill": {
     "duration": 0.143536,
     "end_time": "2022-04-25T21:34:52.618087",
     "exception": false,
     "start_time": "2022-04-25T21:34:52.474551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.6 PLOT EVERY SCENE ALONG WITH COVISIBILITY\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4bfd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:34:52.922157Z",
     "iopub.status.busy": "2022-04-25T21:34:52.921249Z",
     "iopub.status.idle": "2022-04-25T21:35:06.375905Z",
     "shell.execute_reply": "2022-04-25T21:35:06.376341Z",
     "shell.execute_reply.started": "2022-04-11T22:22:03.614561Z"
    },
    "papermill": {
     "duration": 13.61465,
     "end_time": "2022-04-25T21:35:06.376505",
     "exception": false,
     "start_time": "2022-04-25T21:34:52.761855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _s in TRAIN_SCENES:\n",
    "    print(f\"\\n\\n\\nRANDOM PLOT FOR {_s} LOCATION SCENE\")\n",
    "    pco_random_plot(train_map[_s][\"pco_df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c5cd1",
   "metadata": {
    "papermill": {
     "duration": 1.132207,
     "end_time": "2022-04-25T21:35:08.248906",
     "exception": false,
     "start_time": "2022-04-25T21:35:07.116699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.7 EXAMPLE MAPPING KEYPOINTS FROM ONE IMAGE TO ANOTHER (BASIC SIFT)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28355a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:09.613816Z",
     "iopub.status.busy": "2022-04-25T21:35:09.613138Z",
     "iopub.status.idle": "2022-04-25T21:35:10.353122Z",
     "shell.execute_reply": "2022-04-25T21:35:10.353548Z",
     "shell.execute_reply.started": "2022-04-11T22:22:18.028823Z"
    },
    "papermill": {
     "duration": 1.430747,
     "end_time": "2022-04-25T21:35:10.353699",
     "exception": false,
     "start_time": "2022-04-25T21:35:08.922952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1 - Pick An Image Pair And Display\n",
    "DEMO_ROW = train_map[\"taj_mahal\"][\"pco_df\"].iloc[850]\n",
    "DEMO_IMG_ID_1 = DEMO_ROW.image_id_1\n",
    "DEMO_IMG_ID_2 = DEMO_ROW.image_id_2\n",
    "DEMO_PATH_1 = DEMO_ROW.image_path_1\n",
    "DEMO_PATH_2 = DEMO_ROW.image_path_2\n",
    "\n",
    "demo_img_1 = cv2.imread(DEMO_PATH_1)[..., ::-1]\n",
    "demo_img_2 = cv2.imread(DEMO_PATH_2)[..., ::-1]\n",
    "\n",
    "# FM= Fundamental Matrix\n",
    "DEMO_F = arr_from_str(DEMO_ROW.fundamental_matrix)\n",
    "\n",
    "plot_two_paths(DEMO_PATH_1, DEMO_PATH_2, _titles=[f\"IMAGE ID 1: {DEMO_IMG_ID_1}\", f\"IMAGE ID 2: {DEMO_IMG_ID_2}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9192fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:11.812209Z",
     "iopub.status.busy": "2022-04-25T21:35:11.811559Z",
     "iopub.status.idle": "2022-04-25T21:35:13.872642Z",
     "shell.execute_reply": "2022-04-25T21:35:13.873056Z",
     "shell.execute_reply.started": "2022-04-11T22:22:18.807796Z"
    },
    "papermill": {
     "duration": 2.814586,
     "end_time": "2022-04-25T21:35:13.873201",
     "exception": false,
     "start_time": "2022-04-25T21:35:11.058615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2 - Use SIFT (One Approach) To Detect Keypoints In Images \n",
    "\n",
    "#\n",
    "# The task is finding the relative geometry (rotation, translation) between the two cameras.\n",
    "# You can read more about epipolar geometry here: https://en.wikipedia.org/wiki/Epipolar_geometry\n",
    "#\n",
    "# This problem is typically (but not always!) solved with sparse features.\n",
    "# Let's try using SIFT, a seminal work in computer vision (https://en.wikipedia.org/wiki/Scale-invariant_feature_transform).\n",
    "# No longer the state of the art, but still pretty solid!\n",
    "#\n",
    "n_feat = 1_000\n",
    "\n",
    "# About parameter `contrastThreshold`:\n",
    "#     - The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. \n",
    "#     - The larger the threshold, the less features are produced by the detector.\n",
    "contrast_thresh = -10_000\n",
    "\n",
    "# About parameter `edgeThreshold`:\n",
    "#     - The threshold used to filter out edge-like features. \n",
    "#     - Note that the its meaning is different from the contrastThreshold\n",
    "#          --> i.e. the larger the edgeThreshold, the less features are filtered out (more features are retained)\n",
    "edge_thresh = -10_000\n",
    "\n",
    "# You may want to lower the detection threshold, as small images may not be able to reach the budget otherwise.\n",
    "# Note that you may actually get more than num_features features, as a feature for one point can have multiple orientations (this is rare).\n",
    "sift_detector = cv2.SIFT_create(n_feat, contrastThreshold=contrast_thresh, edgeThreshold=edge_thresh)\n",
    "\n",
    "# Leverage sift and capture desired number of keypoints and descriptors\n",
    "#  KEYPOINTS:\n",
    "#     --> The keypoint is characterized by the 2D position, scale (proportional to the \n",
    "#         diameter of the neighborhood that needs to be taken into account), \n",
    "#         orientation and some other parameters. The keypoint neighborhood is then analyzed \n",
    "#         by another algorithm that builds a descriptor (usually represented as a feature vector). \n",
    "#           - the `.pt` attribute yields a tuple indicating the position of the keypoint \n",
    "#             as measured in pixels from the top-left corner of the image (x,y) (see drawn circle center)\n",
    "#           - the `.angle` attribute gives the keypoint orientation (see drawn flag line)\n",
    "#           - the `.size` attribute gives the keypoint magnitude/diameter (see drawn circle size)\n",
    "#           - the `.response` attribute gives the keypoint detector response on the keypoint (strength of the keypoint)\n",
    "#           - the `.octave` attribute gives the pyramid octave in which the keypoint has been detected\n",
    "#\n",
    "#  DESCRIPTORS:\n",
    "#     --> A SIFT descriptor is a 3-D spatial histogram of the image gradients in \n",
    "#         characterizing the appearance of a keypoint. The gradient at each pixel \n",
    "#         is regarded as a sample of a three-dimensional elementary feature vector, \n",
    "#         formed by the pixel location and the gradient orientation. Samples are weighed \n",
    "#         by the gradient norm and accumulated in a 3-D histogram h, which \n",
    "#         (up to normalization and clamping) forms the SIFT descriptor of the region. \n",
    "#         An additional Gaussian weighting function is applied to give less importance to \n",
    "#         gradients farther away from the keypoint center. Orientations are quantized \n",
    "#         into eight bins and the spatial coordinates into four each, as follows.\n",
    "#    --> A SIFT descriptor is essentially a certain number of features describing a keypoint.\n",
    "#        In our case, the number of features is 8*4*4 or 128 features.\n",
    "keypoints_1, descriptors_1 = extract_sift_features(demo_img_1, sift_detector, n_feat)\n",
    "keypoints_2, descriptors_2 = extract_sift_features(demo_img_2, sift_detector, n_feat)\n",
    "\n",
    "# Each local feature contains a keypoint (xy, possibly scale, possibly orientation) and a description vector (128-dimensional for SIFT).\n",
    "#   DRAW_RICH_KEYPOINTS Description: \n",
    "#        --> For each keypoint the circle around keypoint with keypoint size and orientation will be drawn.\n",
    "image_with_keypoints_1 = cv2.drawKeypoints(demo_img_1, keypoints_1, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(image_with_keypoints_1)\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "\n",
    "image_with_keypoints_2 = cv2.drawKeypoints(demo_img_2, keypoints_2, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(image_with_keypoints_2)\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f57faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:15.426954Z",
     "iopub.status.busy": "2022-04-25T21:35:15.426013Z",
     "iopub.status.idle": "2022-04-25T21:35:18.626887Z",
     "shell.execute_reply": "2022-04-25T21:35:18.627287Z",
     "shell.execute_reply.started": "2022-04-11T22:22:20.755608Z"
    },
    "papermill": {
     "duration": 3.98518,
     "end_time": "2022-04-25T21:35:18.627446",
     "exception": false,
     "start_time": "2022-04-25T21:35:14.642266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3 - Match The Keypoints From Image 1 to Image 2 (Many Possible Approaches)\n",
    "\n",
    "# For each descriptor on one image, find the closest descriptor on the other image.\n",
    "# We will leverage a Brute-force descriptor matcher.\n",
    "# NOTE:\n",
    "#     --> With crossCheck=True we keep only bidirectional matches \n",
    "#         i.e. two features are nearest neighbours from A to B and also from B to A\n",
    "bfm = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Compute matches and reduce to [query|train] indexors\n",
    "cv_matches = bfm.match(descriptors_1, descriptors_2)\n",
    "matches_before = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n",
    "\n",
    "# Convert keypoints and matches to something more human-readable.\n",
    "kp_1_pos = np.array([kp.pt for kp in keypoints_1])\n",
    "kp_2_pos = np.array([kp.pt for kp in keypoints_2])\n",
    "kp_matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n",
    "\n",
    "# Plot the brute-force matches.\n",
    "# Notice that this includes many outliers. We can filter them with a state-of-the-art RANSAC algorithm. References:\n",
    "# * https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a\n",
    "# * https://opencv.org/evaluating-opencvs-new-ransacs\n",
    "img_matches_before = draw_cv_matches(demo_img_1, demo_img_2, kp_1_pos, kp_2_pos, matches_before)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.title('Matches BEFORE RANSAC', fontweight=\"bold\")\n",
    "plt.imshow(img_matches_before)\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# OpenCV gives us the Fundamental matrix after RANSAC, and a mask over the input matches. \n",
    "# The solution is clearly much cleaner, even though it may still contain outliers.\n",
    "\n",
    "# This `F` is the prediction you'll submit to the contest.\n",
    "\n",
    "F, inlier_mask = cv2.findFundamentalMat(kp_1_pos[matches_before[:, 0]], kp_2_pos[matches_before[:, 1]], \n",
    "                                        cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n",
    "                                        confidence=0.99999, maxIters=10000)\n",
    "\n",
    "\n",
    "matches_after = np.array([match for match, is_inlier in zip(matches_before, inlier_mask) if is_inlier])\n",
    "img_matches_after = draw_cv_matches(demo_img_1, demo_img_2, kp_1_pos, kp_2_pos, matches_after)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.title('Matches AFTER RANSAC', fontweight=\"bold\")\n",
    "plt.imshow(img_matches_after)\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "\n",
    "# CHECK THE FIRST KEYPOINT\n",
    "match_query_kp_position_1 = keypoints_1[matches_after[0][0]].pt\n",
    "match_query_kp_position_1 = (int(round(match_query_kp_position_1[0])), \n",
    "                             int(round(match_query_kp_position_1[1])))\n",
    "match_train_kp_position_1 = keypoints_2[matches_after[0][1]].pt\n",
    "match_train_kp_position_1 = (int(round(match_train_kp_position_1[0])), \n",
    "                             int(round(match_train_kp_position_1[1])))\n",
    "_tmp_1 = cv2.circle(demo_img_1.copy(), match_query_kp_position_1, 10, (255,0,0), -1)\n",
    "_tmp_2 = cv2.circle(demo_img_2.copy(), match_train_kp_position_1, 10, (255,0,0), -1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"First Keypoint In Image 1 (Query) AFTER RANSAC\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.imshow(_tmp_1)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"First Keypoint In Image 2 (Train) AFTER RANSAC\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.imshow(_tmp_2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# CHECK THE SECOND KEYPOINT\n",
    "match_query_kp_position_2 = keypoints_1[matches_after[1][0]].pt\n",
    "match_query_kp_position_2 = (int(round(match_query_kp_position_2[0])), \n",
    "                             int(round(match_query_kp_position_2[1])))\n",
    "match_train_kp_position_2 = keypoints_2[matches_after[1][1]].pt\n",
    "match_train_kp_position_2 = (int(round(match_train_kp_position_2[0])), \n",
    "                             int(round(match_train_kp_position_2[1])))\n",
    "\n",
    "_tmp_1 = cv2.circle(_tmp_1, match_query_kp_position_2, 10, (0,255,0), -1)\n",
    "_tmp_2 = cv2.circle(_tmp_2, match_train_kp_position_2, 10, (0,255,0), -1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Second Keypoint In Image 1 (Query) AFTER RANSAC\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.imshow(_tmp_1)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Second Keypoint In Image 2 (Train) AFTER RANSAC\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.imshow(_tmp_2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# CHECK THE THIRD KEYPOINT\n",
    "match_query_kp_position_3 = keypoints_1[matches_after[2][0]].pt\n",
    "match_query_kp_position_3 = (int(round(match_query_kp_position_3[0])), \n",
    "                             int(round(match_query_kp_position_3[1])))\n",
    "match_train_kp_position_3 = keypoints_2[matches_after[2][1]].pt\n",
    "match_train_kp_position_3 = (int(round(match_train_kp_position_3[0])), \n",
    "                             int(round(match_train_kp_position_3[1])))\n",
    "\n",
    "_tmp_1 = cv2.circle(_tmp_1, match_query_kp_position_3, 10, (0,0,255), -1)\n",
    "_tmp_2 = cv2.circle(_tmp_2, match_train_kp_position_3, 10, (0,0,255), -1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Third Keypoint In Image 1 (Query) AFTER RANSAC\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.imshow(_tmp_1)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Third Keypoint In Image 2 (Train) AFTER RANSAC\", fontweight=\"bold\")\n",
    "plt.axis(False)\n",
    "plt.imshow(_tmp_2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a25d3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:20.416446Z",
     "iopub.status.busy": "2022-04-25T21:35:20.415829Z",
     "iopub.status.idle": "2022-04-25T21:35:20.432741Z",
     "shell.execute_reply": "2022-04-25T21:35:20.432288Z",
     "shell.execute_reply.started": "2022-04-11T22:22:24.053476Z"
    },
    "papermill": {
     "duration": 0.927539,
     "end_time": "2022-04-25T21:35:20.432870",
     "exception": false,
     "start_time": "2022-04-25T21:35:19.505331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4 - See How We Did #\n",
    "\n",
    "# One important caveat: the scenes were reconstructed from unstructured image collections using \n",
    "# Structure-from-Motion (http://colmap.github.io), and are not up to \"real-world\" scale (i.e. meters, or inches).\n",
    "#\n",
    "# The hosts have computed a scaling factor per scene to correct this. \n",
    "# This is necessary to compute the metric correctly.\n",
    "DEMO_SCALE_FACTOR = scale_map[\"taj_mahal\"]\n",
    "print(f\"The Taj Mahal Scaling Factor is: {DEMO_SCALE_FACTOR}\")\n",
    "\n",
    "inlier_kp_1 = np.array([kp.pt for i, kp in enumerate(keypoints_1) if i in matches_after[:, 0]])\n",
    "inlier_kp_2 = np.array([kp.pt for i, kp in enumerate(keypoints_2) if i in matches_after[:, 1]])\n",
    "\n",
    "E, R, T = compute_essential_matrix(F, id2cal_map[DEMO_IMG_ID_1][\"camera_intrinsics\"], id2cal_map[DEMO_IMG_ID_2][\"camera_intrinsics\"], inlier_kp_1, inlier_kp_2)\n",
    "q = quaternion_from_matrix(R)\n",
    "T = T.flatten()\n",
    "\n",
    "# Get the ground truth relative pose difference for this pair of images.\n",
    "R1_gt = id2cal_map[DEMO_IMG_ID_1][\"rotation_matrix\"]\n",
    "T1_gt = id2cal_map[DEMO_IMG_ID_1][\"translation_vector\"].reshape((3, 1))\n",
    "\n",
    "R2_gt = id2cal_map[DEMO_IMG_ID_2][\"rotation_matrix\"]\n",
    "T2_gt = id2cal_map[DEMO_IMG_ID_2][\"translation_vector\"].reshape((3, 1))\n",
    "\n",
    "dR_gt = np.dot(R2_gt, R1_gt.T)\n",
    "dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n",
    "\n",
    "q_gt = quaternion_from_matrix(dR_gt)\n",
    "q_gt = q_gt / (np.linalg.norm(q_gt) + 1e-15)\n",
    "\n",
    "# Given ground truth and prediction, compute the error for the example above.\n",
    "err_q, err_t = compute_error_for_example(q_gt, dT_gt, q, T, DEMO_SCALE_FACTOR)\n",
    "\n",
    "print(f'rotation_error={err_q:.02f} (deg), translation_error={err_t:.02f} (m)', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f36d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T16:12:49.195617Z",
     "iopub.status.busy": "2022-03-09T16:12:49.194824Z",
     "iopub.status.idle": "2022-03-09T16:12:49.202167Z",
     "shell.execute_reply": "2022-03-09T16:12:49.200859Z",
     "shell.execute_reply.started": "2022-03-09T16:12:49.195577Z"
    },
    "papermill": {
     "duration": 0.869885,
     "end_time": "2022-04-25T21:35:22.173980",
     "exception": false,
     "start_time": "2022-04-25T21:35:21.304095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.MODEL BASELINE\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2e9bf",
   "metadata": {
    "papermill": {
     "duration": 0.90348,
     "end_time": "2022-04-25T21:35:23.955328",
     "exception": false,
     "start_time": "2022-04-25T21:35:23.051848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.1 BASELINE USING SIMPLE\n",
    "---\n",
    "\n",
    "Let's create a wrapper function that can take an image pair and will return an approximate F matrix so we can submit using the basic techniques illustrated by the hosts in this [notebook ](https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?rvi=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2d0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:26.034986Z",
     "iopub.status.busy": "2022-04-25T21:35:26.034063Z",
     "iopub.status.idle": "2022-04-25T21:35:26.035898Z",
     "shell.execute_reply": "2022-04-25T21:35:26.036617Z",
     "shell.execute_reply.started": "2022-04-11T22:23:42.042787Z"
    },
    "papermill": {
     "duration": 0.944605,
     "end_time": "2022-04-25T21:35:26.036763",
     "exception": false,
     "start_time": "2022-04-25T21:35:25.092158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_N_FEATURES      = 5_000\n",
    "INFERENCE_CONTRAST_THRESH = -10_000\n",
    "INFERENCE_EDGE_THRESH     = -10_000\n",
    "INFERENCE_N_OCTAVES       = 3\n",
    "DETECTOR_STYLE            = \"SIFT\" # [\"SIFT\"|\"ORB\"]\n",
    "MATCHER_STYLE             = \"BFM\"\n",
    "\n",
    "# IF USING ORB WE ALSO USE BEBLID\n",
    "#\n",
    "# BEBLID (Boosted Efficient Binary Local Image Descriptor): \n",
    "#           - A new descriptor introduced in 2020 that has been shown to improve ORB \n",
    "#             in several tasks. Since BEBLID works for several detection methods, \n",
    "#             you have to set the scale for the ORB keypoints which is 0.75~1. \n",
    "if DETECTOR_STYLE==\"SIFT\":\n",
    "    INFERENCE_DETECTOR = cv2.SIFT_create(INFERENCE_N_FEATURES, INFERENCE_N_OCTAVES, INFERENCE_CONTRAST_THRESH, INFERENCE_EDGE_THRESH)\n",
    "    INFERENCE_DESCRIPTOR = None\n",
    "elif DETECTOR_STYLE==\"ORB\":\n",
    "    INFERENCE_DETECTOR = cv2.ORB_create(INFERENCE_N_FEATURES, edgeThreshold=INFERENCE_EDGE_THRESH)\n",
    "    INFERENCE_DESCRIPTOR = cv2.xfeatures2d.BEBLID_create(0.75)\n",
    "else:        \n",
    "    raise NotImplementedError(\"Only SIFT and ORB are Implemented So Far\")\n",
    "    \n",
    "if MATCHER_STYLE==\"BFM\":\n",
    "    INFERENCE_MATCHER = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "else:\n",
    "    raise NotImplementedError(\"Only BruteForce Is Implemented So Far\")\n",
    "\n",
    "# We know there are ~10000 pairs of images so ideally we want this to run pretty quick\n",
    "def get_fundamental_matrix_v1(img_path_1, img_path_2, detector, matcher, descriptor=None, n_feat=10_000, fancy_matching=False):\n",
    "    # Step 1 - Load the Images\n",
    "    img_1 = cv2.imread(img_path_1)[..., ::-1]\n",
    "    img_2 = cv2.imread(img_path_2)[..., ::-1]\n",
    "    \n",
    "    # Step 2 - Get KPs and Descriptors (Slowest Step --> ~0.6-0.8 seconds)\n",
    "    if descriptor is None:\n",
    "        keypoints_1, descriptors_1 = extract_sift_features(img_1, detector, n_feat)\n",
    "        keypoints_2, descriptors_2 = extract_sift_features(img_2, detector, n_feat)\n",
    "    else:\n",
    "        keypoints_1, descriptors_1 = descriptor.compute(img_1, extract_keypoints(img_1, detector))\n",
    "        keypoints_1, descriptors_1 = keypoints_1[:n_feat], descriptors_1[:n_feat]\n",
    "        \n",
    "        keypoints_2, descriptors_2 = descriptor.compute(img_2, extract_keypoints(img_2, detector))\n",
    "        keypoints_2, descriptors_2 = keypoints_2[:n_feat], descriptors_2[:n_feat]\n",
    "    \n",
    "    # Step 3 - Get position information from Keypoint objects\n",
    "    kp_1_pos = np.array([kp.pt for kp in keypoints_1])\n",
    "    kp_2_pos = np.array([kp.pt for kp in keypoints_2])\n",
    "    \n",
    "    # Step 4 - Compute Matches (Before RANSAC)(2nd Slowest Step --> ~0.1-0.2 seconds)\n",
    "    \n",
    "    if not fancy_matching:\n",
    "        matches = matcher.match(descriptors_1, descriptors_2)\n",
    "        matches = np.array([[m.queryIdx, m.trainIdx] for m in matches])\n",
    "    else:\n",
    "        matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
    "        nn_matches = matcher.knnMatch(descriptors_1, descriptors_2, 2)\n",
    "        matches = np.array([[m.queryIdx, m.trainIdx] for m,n in nn_matches if m.distance<(0.8*n.distance)])\n",
    "        \n",
    "    # Step 5 - Use CV2 To Perform RANSAC and Get the Fundamental Matrix\n",
    "    F, inlier_mask = cv2.findFundamentalMat(kp_1_pos[matches[:, 0]], kp_2_pos[matches[:, 1]], \n",
    "                                            cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n",
    "                                            confidence=0.99999, maxIters=15000)\n",
    "    \n",
    "    # Step 6 - Convert our array to a flattened string \n",
    "    return \" \".join([str(x) for x in F.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186b696",
   "metadata": {
    "papermill": {
     "duration": 0.901735,
     "end_time": "2022-04-25T21:35:27.881795",
     "exception": false,
     "start_time": "2022-04-25T21:35:26.980060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.2 DELF BASELINE\n",
    "---\n",
    "\n",
    "The DELF module takes an image as input and will describe noteworthy points with vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacb3c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:29.662285Z",
     "iopub.status.busy": "2022-04-25T21:35:29.661275Z",
     "iopub.status.idle": "2022-04-25T21:35:50.835597Z",
     "shell.execute_reply": "2022-04-25T21:35:50.835076Z",
     "shell.execute_reply.started": "2022-04-11T23:33:30.348551Z"
    },
    "papermill": {
     "duration": 22.082368,
     "end_time": "2022-04-25T21:35:50.835749",
     "exception": false,
     "start_time": "2022-04-25T21:35:28.753381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the DELF constants... shown raw below\n",
    "DELF_SCALES = tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0])\n",
    "DELF_SCORE_THRESH = tf.constant(100.0)\n",
    "DELF_MAX_FEATURES = tf.constant(2500)\n",
    "\n",
    "# Prepare an image tensor.\n",
    "tf_demo_img_1 = tf.cast(demo_img_1, tf.float32)\n",
    "tf_demo_img_2 = tf.cast(demo_img_2, tf.float32)\n",
    "\n",
    "# Instantiate the DELF module using the offline directory\n",
    "LOCAL_TFHUB_DELF_PATH = \"/kaggle/input/offline-delf-from-tfhub/delf_cache/333559635dbfa88957ed2e7ed45bdbfe3353e356\"\n",
    "delf_module = tfhub.load(LOCAL_TFHUB_DELF_PATH).signatures['default']\n",
    "\n",
    "delf_inputs_1 = {\n",
    "  # An image tensor with dtype float32 and shape [height, width, 3], where\n",
    "  # height and width are positive integers:\n",
    "  'image': tf_demo_img_1,\n",
    "    \n",
    "  # Scaling factors for building the image pyramid as described in the paper:\n",
    "  'image_scales': tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0]),\n",
    "    \n",
    "  # Image features whose attention score exceeds this threshold will be returned:\n",
    "  'score_threshold': tf.constant(100.0),\n",
    "    \n",
    "  # The maximum number of features that should be returned:\n",
    "  'max_feature_num': tf.constant(1000),\n",
    "}\n",
    "\n",
    "# Apply the DELF module to the inputs to get the outputs.\n",
    "#\n",
    "# NOTE: \n",
    "#  delf_outputs is a dictionary of named tensors:\n",
    "#      * delf_outputs['locations']: \n",
    "#           - a Tensor with dtype float32 and shape [None, 2],\n",
    "#           - where each entry is a coordinate (vertical-offset, horizontal-offset) \n",
    "#             in pixels from the top-left corner of the image.\n",
    "#      * delf_outputs['descriptors']: \n",
    "#.          - a Tensor with dtype float32 and shape [None, 40], \n",
    "#           - where delf_outputs['descriptors'][i] is a 40-dimensional\n",
    "#             descriptor for the image at location delf_outputs['locations'][i]\n",
    "delf_outputs_1 = delf_module(**delf_inputs_1)\n",
    "\n",
    "delf_inputs_2 = {\"image\":tf_demo_img_2, \n",
    "                 \"image_scales\":DELF_SCALES, \n",
    "                 \"score_threshold\":DELF_SCORE_THRESH,\n",
    "                 \"max_feature_num\":DELF_MAX_FEATURES}\n",
    "delf_outputs_2 = delf_module(**delf_inputs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3ec89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:35:52.647647Z",
     "iopub.status.busy": "2022-04-25T21:35:52.646837Z",
     "iopub.status.idle": "2022-04-25T21:36:13.193006Z",
     "shell.execute_reply": "2022-04-25T21:36:13.193402Z",
     "shell.execute_reply.started": "2022-04-12T00:13:16.400839Z"
    },
    "papermill": {
     "duration": 21.462295,
     "end_time": "2022-04-25T21:36:13.193573",
     "exception": false,
     "start_time": "2022-04-25T21:35:51.731278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delf_inference(img, delf_model,\n",
    "                   _scales=(0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0),\n",
    "                   _score_thresh=100.0, _n_features=2500,):\n",
    "    tf_img = tf.cast(img, tf.float32)\n",
    "    input_map = {\"image\":tf_img, \n",
    "                 \"image_scales\":tf.constant(_scales), \n",
    "                 \"score_threshold\":tf.constant(_score_thresh),\n",
    "                 \"max_feature_num\":tf.constant(_n_features)}\n",
    "    outputs = delf_model(**input_map)\n",
    "    keypoints, descriptors = outputs[\"locations\"].numpy(), outputs[\"descriptors\"].numpy()    \n",
    "    keypoints[:, [0, 1]] = keypoints[:, [1, 0]]\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "def use_nn_to_filter_kps(keypoints_1, keypoints_2, descriptors_1, descriptors_2, d_thresh=0.8):\n",
    "    n_kp_1, n_kp_2 = keypoints_1.shape[0], keypoints_2.shape[0]\n",
    "    \n",
    "    # Find nearest-neighbor matches using a KD tree.\n",
    "    d1_tree = cKDTree(descriptors_1)\n",
    "    _, idxs = d1_tree.query(descriptors_2, distance_upper_bound=d_thresh)\n",
    "\n",
    "    # Select feature locations for putative matches.\n",
    "    keypoints_2 = np.array([keypoints_2[i,] for i in range(n_kp_2) if idxs[i]!=n_kp_1])\n",
    "    keypoints_1 = np.array([keypoints_1[idxs[i],] for i in range(n_kp_2) if idxs[i]!=n_kp_1])\n",
    "    \n",
    "    matches = np.array([(i,i) for i in range(keypoints_1.shape[0])])\n",
    "    return keypoints_1, keypoints_2, matches\n",
    "\n",
    "def get_fundamental_matrix_v2(img_path_1, img_path_2, detector, matcher=\"nn\", n_feat=10_000, do_plots=False):\n",
    "    # Step 1 - Load the Images\n",
    "    img_1 = cv2.imread(img_path_1)[..., ::-1]/255.\n",
    "    img_2 = cv2.imread(img_path_2)[..., ::-1]/255.\n",
    "    \n",
    "    # Step 2 - Get KPs and Descriptors\n",
    "    kp_1_pos, descriptors_1 = delf_inference(img_1, detector, _n_features=n_feat)\n",
    "    kp_2_pos, descriptors_2 = delf_inference(img_2, detector, _n_features=n_feat)\n",
    "        \n",
    "    # Step 3 - Compute Matches (Before RANSAC)(2nd Slowest Step --> ~0.1-0.2 seconds)\n",
    "    if matcher==\"nn\":\n",
    "        kp_1_pos, kp_2_pos, matches = use_nn_to_filter_kps(kp_1_pos, kp_2_pos, \n",
    "                                                           descriptors_1, descriptors_2)\n",
    "    else:\n",
    "        matches = matcher.match(descriptors_1, descriptors_2)\n",
    "        matches = np.array([[m.queryIdx, m.trainIdx] for m in matches])\n",
    "        \n",
    "    if do_plots:\n",
    "        img_matches_before = draw_cv_matches(255*img_1.copy(), 255*img_2.copy(), kp_1_pos, kp_2_pos, matches)\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.title('Matches BEFORE RANSAC', fontweight=\"bold\")\n",
    "        plt.imshow(img_matches_before)\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "        \n",
    "    # Step 4 - Use CV2 To Perform RANSAC and Get the Fundamental Matrix\n",
    "    F, inlier_mask = cv2.findFundamentalMat(kp_1_pos[matches[:, 0]], kp_2_pos[matches[:, 1]], \n",
    "                                            cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n",
    "                                            confidence=0.99999, maxIters=100_000)\n",
    "    \n",
    "    if do_plots:\n",
    "        matches = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n",
    "        img_matches_after = draw_cv_matches(255*img_1.copy(), 255*img_2.copy(), kp_1_pos, kp_2_pos, matches)\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.title('Matches AFTER RANSAC', fontweight=\"bold\")\n",
    "        plt.imshow(img_matches_after)\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "    \n",
    "    # Step 6 - Convert our array to a flattened string \n",
    "    return \" \".join([str(x) for x in F.flatten()])\n",
    "\n",
    "print(\"\\n\\nDEMO WITH REGULAR MATCHER\\n\")\n",
    "DEMO_F = get_fundamental_matrix_v2(DEMO_PATH_1, DEMO_PATH_2, detector=delf_module, matcher=INFERENCE_MATCHER, n_feat=500, do_plots=True)\n",
    "\n",
    "print(\"\\n\\nDEMO WITH KNN MATCHER\\n\")\n",
    "DEMO_F = get_fundamental_matrix_v2(DEMO_PATH_1, DEMO_PATH_2, detector=delf_module, matcher=\"nn\", n_feat=500, do_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387f145",
   "metadata": {
    "papermill": {
     "duration": 0.958822,
     "end_time": "2022-04-25T21:36:15.115366",
     "exception": false,
     "start_time": "2022-04-25T21:36:14.156544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. SUBMISSION\n",
    "---\n",
    "\n",
    "**For this submission I will use the KNN matcher and the newly minted DELF module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28ec84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T21:36:17.101328Z",
     "iopub.status.busy": "2022-04-25T21:36:17.100605Z",
     "iopub.status.idle": "2022-04-25T21:36:25.008542Z",
     "shell.execute_reply": "2022-04-25T21:36:25.007983Z",
     "shell.execute_reply.started": "2022-04-12T00:13:28.06209Z"
    },
    "papermill": {
     "duration": 8.910848,
     "end_time": "2022-04-25T21:36:25.008675",
     "exception": false,
     "start_time": "2022-04-25T21:36:16.097827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_test_image_paths_1 = test_df.f_path_1.tolist()\n",
    "all_test_image_paths_2 = test_df.f_path_2.tolist()\n",
    "\n",
    "if len(all_test_image_paths_1)==3:\n",
    "    ss_df[\"fundamental_matrix\"] = [get_fundamental_matrix_v2(_path_1, _path_2, detector=delf_module, matcher=\"nn\", do_plots=True) for _path_1, _path_2 in zip(all_test_image_paths_1, all_test_image_paths_2)]\n",
    "else:\n",
    "    ss_df[\"fundamental_matrix\"] = [get_fundamental_matrix_v2(_path_1, _path_2, detector=delf_module, matcher=INFERENCE_MATCHER) for _path_1, _path_2 in zip(all_test_image_paths_1, all_test_image_paths_2)]\n",
    "    \n",
    "display(ss_df)\n",
    "ss_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae02d08",
   "metadata": {
    "papermill": {
     "duration": 1.189306,
     "end_time": "2022-04-25T21:36:27.425523",
     "exception": false,
     "start_time": "2022-04-25T21:36:26.236217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.176314,
   "end_time": "2022-04-25T21:36:31.711801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-25T21:34:08.535487",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07cd6edf93d0485ea939220ccf97339b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b57046b6fa214748854574c39d6b1d4c",
       "max": 16,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_de5af324844c4be78416dcc92a66f1ea",
       "value": 16
      }
     },
     "194f94bd8ac64f1b90167c5d6508ce7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d95813fa59348d9b28dc446d22e968a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30c2226f26d140e9a449fa12b7280d10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_79af13f71ddd482eb6a31b9003a916f6",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_c7d984ab78dd4af3a279a4a150ba0050",
       "value": "100%"
      }
     },
     "3d6c50520a774b7b9bcaf9a89c60a4b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "40b3d42fbbba4f50acdcb9b4483e9656": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "457d256dbec348e18bbf6e01ae2d06e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d0a5b85ee1804e179923af0308f87873",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_af574a36540a44e7a5c5960583b5402c",
       "value": "100%"
      }
     },
     "72a61847efb1459f8d280986b03c7c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_40b3d42fbbba4f50acdcb9b4483e9656",
       "max": 16,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3d6c50520a774b7b9bcaf9a89c60a4b9",
       "value": 16
      }
     },
     "79af13f71ddd482eb6a31b9003a916f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "812f413559964e8e8bc4e3e6c140e83c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_30c2226f26d140e9a449fa12b7280d10",
        "IPY_MODEL_07cd6edf93d0485ea939220ccf97339b",
        "IPY_MODEL_bc164b6af47c4e6395282e87a9d5a40e"
       ],
       "layout": "IPY_MODEL_194f94bd8ac64f1b90167c5d6508ce7b"
      }
     },
     "9649910afa1447f9a3b97b45844afe2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a65a9aa0708e47d98be7a0f8ef5b6326": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d95813fa59348d9b28dc446d22e968a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ffc9454147f146ae93d2ca973922417f",
       "value": " 16/16 [00:00&lt;00:00, 36.91it/s]"
      }
     },
     "af574a36540a44e7a5c5960583b5402c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b57046b6fa214748854574c39d6b1d4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc164b6af47c4e6395282e87a9d5a40e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f38361071ea44bae8934860ef0bea61c",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_f4ac243b15b3428b95b5c2a8e821ef07",
       "value": " 16/16 [00:15&lt;00:00,  1.20s/it]"
      }
     },
     "c7d984ab78dd4af3a279a4a150ba0050": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d0a5b85ee1804e179923af0308f87873": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de5af324844c4be78416dcc92a66f1ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f38361071ea44bae8934860ef0bea61c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4ac243b15b3428b95b5c2a8e821ef07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f53b2fcdef42417fbe73937be442d0c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_457d256dbec348e18bbf6e01ae2d06e3",
        "IPY_MODEL_72a61847efb1459f8d280986b03c7c81",
        "IPY_MODEL_a65a9aa0708e47d98be7a0f8ef5b6326"
       ],
       "layout": "IPY_MODEL_9649910afa1447f9a3b97b45844afe2a"
      }
     },
     "ffc9454147f146ae93d2ca973922417f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
